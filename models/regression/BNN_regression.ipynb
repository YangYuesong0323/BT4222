{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Bitcoin price prediction with Bayesian Neural Network Regression using torchBNN and PyTorch.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "import datetime\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torchinfo import summary\n",
    "import torchbnn as bnn\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import r2_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "cwd = os.getcwd()\n",
    "from os.path import dirname, abspath\n",
    "while not cwd.endswith('BT4222_repo'):\n",
    "    cwd = os.path.dirname(cwd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = os.path.join(cwd,'data','cooked_data','cooked_complete_dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>Adj_Close_BTC-USD</th>\n",
       "      <th>Open_BTC-USD</th>\n",
       "      <th>High_BTC-USD</th>\n",
       "      <th>Low_BTC-USD</th>\n",
       "      <th>Volume_BTC-USD</th>\n",
       "      <th>Adj_Close_SPY</th>\n",
       "      <th>Adj_Close_GLD</th>\n",
       "      <th>Adj_Close_CHFUSD=X</th>\n",
       "      <th>Adj_Close_CNYUSD=X</th>\n",
       "      <th>Adj_Close_EURUSD=X</th>\n",
       "      <th>Adj_Close_GBPUSD=X</th>\n",
       "      <th>Adj_Close_JPYUSD=X</th>\n",
       "      <th>coindesk_sentiment</th>\n",
       "      <th>num_of_coindesk_posts</th>\n",
       "      <th>reddit_comments_sentiments</th>\n",
       "      <th>top_50_reddit_posts_sentiments</th>\n",
       "      <th>blockchain_transactions_per_block</th>\n",
       "      <th>blockchain_hash_rates</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>14/12/20</td>\n",
       "      <td>19246.64453</td>\n",
       "      <td>19144.49219</td>\n",
       "      <td>19305.09961</td>\n",
       "      <td>19012.70898</td>\n",
       "      <td>2.247400e+10</td>\n",
       "      <td>361.926788</td>\n",
       "      <td>171.539993</td>\n",
       "      <td>1.125442</td>\n",
       "      <td>0.152772</td>\n",
       "      <td>1.213340</td>\n",
       "      <td>1.331824</td>\n",
       "      <td>0.009621</td>\n",
       "      <td>0.249489</td>\n",
       "      <td>12.0</td>\n",
       "      <td>0.158060</td>\n",
       "      <td>0.677618</td>\n",
       "      <td>2167.931034</td>\n",
       "      <td>134574371.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>15/12/20</td>\n",
       "      <td>19417.07617</td>\n",
       "      <td>19246.91992</td>\n",
       "      <td>19525.00781</td>\n",
       "      <td>19079.84180</td>\n",
       "      <td>2.674198e+10</td>\n",
       "      <td>366.819824</td>\n",
       "      <td>173.940002</td>\n",
       "      <td>1.127930</td>\n",
       "      <td>0.152679</td>\n",
       "      <td>1.214890</td>\n",
       "      <td>1.333084</td>\n",
       "      <td>0.009614</td>\n",
       "      <td>0.173773</td>\n",
       "      <td>18.0</td>\n",
       "      <td>0.101930</td>\n",
       "      <td>0.447277</td>\n",
       "      <td>2288.857143</td>\n",
       "      <td>129933875.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>16/12/20</td>\n",
       "      <td>21310.59766</td>\n",
       "      <td>19418.81836</td>\n",
       "      <td>21458.90820</td>\n",
       "      <td>19298.31641</td>\n",
       "      <td>4.440901e+10</td>\n",
       "      <td>367.395508</td>\n",
       "      <td>174.899994</td>\n",
       "      <td>1.129382</td>\n",
       "      <td>0.152945</td>\n",
       "      <td>1.215430</td>\n",
       "      <td>1.344447</td>\n",
       "      <td>0.009649</td>\n",
       "      <td>0.341491</td>\n",
       "      <td>11.0</td>\n",
       "      <td>0.127344</td>\n",
       "      <td>0.480809</td>\n",
       "      <td>2204.314685</td>\n",
       "      <td>132718173.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>17/12/20</td>\n",
       "      <td>22805.16211</td>\n",
       "      <td>21308.35156</td>\n",
       "      <td>23642.66016</td>\n",
       "      <td>21234.67578</td>\n",
       "      <td>7.137861e+10</td>\n",
       "      <td>369.449982</td>\n",
       "      <td>176.740005</td>\n",
       "      <td>1.129446</td>\n",
       "      <td>0.153109</td>\n",
       "      <td>1.219959</td>\n",
       "      <td>1.350293</td>\n",
       "      <td>0.009664</td>\n",
       "      <td>0.197572</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.135945</td>\n",
       "      <td>0.539729</td>\n",
       "      <td>2399.077519</td>\n",
       "      <td>119724785.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>18/12/20</td>\n",
       "      <td>23137.96094</td>\n",
       "      <td>22806.79688</td>\n",
       "      <td>23238.60156</td>\n",
       "      <td>22399.81250</td>\n",
       "      <td>4.038790e+10</td>\n",
       "      <td>367.974792</td>\n",
       "      <td>176.440002</td>\n",
       "      <td>1.130301</td>\n",
       "      <td>0.153090</td>\n",
       "      <td>1.226272</td>\n",
       "      <td>1.357018</td>\n",
       "      <td>0.009696</td>\n",
       "      <td>0.315601</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.135441</td>\n",
       "      <td>0.449503</td>\n",
       "      <td>2392.031847</td>\n",
       "      <td>145711560.8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       date  Adj_Close_BTC-USD  Open_BTC-USD  High_BTC-USD  Low_BTC-USD  \\\n",
       "1  14/12/20        19246.64453   19144.49219   19305.09961  19012.70898   \n",
       "2  15/12/20        19417.07617   19246.91992   19525.00781  19079.84180   \n",
       "3  16/12/20        21310.59766   19418.81836   21458.90820  19298.31641   \n",
       "4  17/12/20        22805.16211   21308.35156   23642.66016  21234.67578   \n",
       "5  18/12/20        23137.96094   22806.79688   23238.60156  22399.81250   \n",
       "\n",
       "   Volume_BTC-USD  Adj_Close_SPY  Adj_Close_GLD  Adj_Close_CHFUSD=X  \\\n",
       "1    2.247400e+10     361.926788     171.539993            1.125442   \n",
       "2    2.674198e+10     366.819824     173.940002            1.127930   \n",
       "3    4.440901e+10     367.395508     174.899994            1.129382   \n",
       "4    7.137861e+10     369.449982     176.740005            1.129446   \n",
       "5    4.038790e+10     367.974792     176.440002            1.130301   \n",
       "\n",
       "   Adj_Close_CNYUSD=X  Adj_Close_EURUSD=X  Adj_Close_GBPUSD=X  \\\n",
       "1            0.152772            1.213340            1.331824   \n",
       "2            0.152679            1.214890            1.333084   \n",
       "3            0.152945            1.215430            1.344447   \n",
       "4            0.153109            1.219959            1.350293   \n",
       "5            0.153090            1.226272            1.357018   \n",
       "\n",
       "   Adj_Close_JPYUSD=X  coindesk_sentiment  num_of_coindesk_posts  \\\n",
       "1            0.009621            0.249489                   12.0   \n",
       "2            0.009614            0.173773                   18.0   \n",
       "3            0.009649            0.341491                   11.0   \n",
       "4            0.009664            0.197572                   10.0   \n",
       "5            0.009696            0.315601                    2.0   \n",
       "\n",
       "   reddit_comments_sentiments  top_50_reddit_posts_sentiments  \\\n",
       "1                    0.158060                        0.677618   \n",
       "2                    0.101930                        0.447277   \n",
       "3                    0.127344                        0.480809   \n",
       "4                    0.135945                        0.539729   \n",
       "5                    0.135441                        0.449503   \n",
       "\n",
       "   blockchain_transactions_per_block  blockchain_hash_rates  \n",
       "1                        2167.931034            134574371.4  \n",
       "2                        2288.857143            129933875.8  \n",
       "3                        2204.314685            132718173.2  \n",
       "4                        2399.077519            119724785.6  \n",
       "5                        2392.031847            145711560.8  "
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(PATH)\n",
    "df.dropna(inplace = True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['date'] = df['date'].apply(lambda x: datetime.datetime.strptime(x, \"%d/%m/%y\"))\n",
    "df[\"date\"] = pd.to_datetime(df[\"date\"], format='%d/%m/%Y', infer_datetime_format=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Helper Functions for creating lags and scaling**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lag(data, dic):\n",
    "    cols = []\n",
    "    for key, value in dic.items():\n",
    "        for i in range(1, value+1):\n",
    "            cols.append(data[key].shift(i).rename('{}_lag{}'.format(data[key].name, i)))\n",
    "    return pd.concat([data[\"date\"],data[\"Adj_Close_BTC-USD\"]] + cols, axis = 1)\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "def scale_and_convert_to_tensor(Xtrain, Xtest, Ytrain, Ytest, scaleTarget = False):  \n",
    "    global scaler\n",
    "    \n",
    "    # Standardise features\n",
    "    Xtrain_standardised = scaler.fit_transform(Xtrain)\n",
    "    Xtest_standardised = scaler.transform(Xtest)\n",
    "    \n",
    "    # Standardise target\n",
    "    Ytrain_standardised = Ytrain\n",
    "    Ytest_standardised = Ytest\n",
    "    \n",
    "    if scaleTarget:    \n",
    "        Ytrain_standardised = scaler.fit_transform(np.array(Ytrain).reshape(-1, 1))\n",
    "        Ytest_standardised = scaler.transform(np.array(Ytest).reshape(-1, 1))\n",
    "    \n",
    "    ## Change to tensor\n",
    "    Xtrain_tensor = torch.from_numpy(Xtrain_standardised).float()\n",
    "    Ytrain_tensor = torch.from_numpy(np.array(Ytrain_standardised)).float()\n",
    "    Xtest_tensor = torch.from_numpy(Xtest_standardised).float()\n",
    "    Ytest_tensor = torch.from_numpy(np.array(Ytest_standardised)).float()\n",
    "        \n",
    "    return (Xtrain_tensor, Xtest_tensor, Ytrain_tensor, Ytest_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Without Sentiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1. Create feature lags**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_lags = {\"Adj_Close_BTC-USD\" : 1,\n",
    "                \"Adj_Close_SPY\" : 1,\n",
    "                \"Adj_Close_GLD\" : 1,\n",
    "                \"Adj_Close_CHFUSD=X\" : 1,\n",
    "                \"Adj_Close_EURUSD=X\" : 1,\n",
    "                \"Adj_Close_GBPUSD=X\" : 1,\n",
    "                \"Adj_Close_JPYUSD=X\" : 1,\n",
    "                \"blockchain_transactions_per_block\" : 1,\n",
    "                \"blockchain_hash_rates\" : 1}\n",
    "\n",
    "data = lag(df, feature_lags).drop_duplicates(subset = 'date')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2. Handle train-test split**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data[(data[\"date\"] >= \"2021-01-01\") & (data[\"date\"] <= \"2021-04-12\")]\n",
    "\n",
    "## train, validation and test split\n",
    "train = data.loc[(data[\"date\"] >= \"2021-01-01\") & (data[\"date\"] <= \"2021-03-14\")]\n",
    "validation = data[(data[\"date\"] >= \"2021-03-15\") & (data[\"date\"] <= \"2021-03-29\")]\n",
    "test = data[(data[\"date\"] >= \"2021-03-30\") & (data[\"date\"] <= \"2021-04-12\")]\n",
    "\n",
    "## refit and full for later use \n",
    "refit = data[(data[\"date\"] >= \"2021-01-01\") & (data[\"date\"] <= \"2021-03-29\")]\n",
    "full = data.copy(deep = True)\n",
    "\n",
    "## train \n",
    "X_train = train.drop([\"date\", \"Adj_Close_BTC-USD\"], axis = 1)\n",
    "y_train = train[\"Adj_Close_BTC-USD\"]\n",
    "\n",
    "## validation\n",
    "X_val = validation.drop([\"date\", \"Adj_Close_BTC-USD\"], axis = 1)\n",
    "y_val = validation[\"Adj_Close_BTC-USD\"]\n",
    "\n",
    "## test\n",
    "X_test = test.drop([\"date\", \"Adj_Close_BTC-USD\"], axis = 1)\n",
    "y_test = test[\"Adj_Close_BTC-USD\"]\n",
    "\n",
    "## refit\n",
    "X_refit = refit.drop([\"date\", \"Adj_Close_BTC-USD\"], axis = 1)\n",
    "y_refit = refit[\"Adj_Close_BTC-USD\"]\n",
    "\n",
    "## full\n",
    "X_full = full.drop([\"date\", \"Adj_Close_BTC-USD\"], axis = 1)\n",
    "y_full = full[\"Adj_Close_BTC-USD\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3. Standardise dataset and transform into tensors for pytorch**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X train tensor torch.Size([73, 9])\n",
      "Y train tensor torch.Size([73, 1])\n",
      "X val tensor torch.Size([15, 9])\n",
      "Y val tensor torch.Size([15, 1])\n",
      "X test tensor torch.Size([14, 9])\n",
      "Y test tensor torch.Size([14, 1])\n"
     ]
    }
   ],
   "source": [
    "## Standardise datasets and convert into tensors\n",
    "Xtrain_tensor, Xval_tensor, Ytrain_tensor, Yval_tensor = scale_and_convert_to_tensor(X_train, X_val, y_train, y_val, scaleTarget = True)\n",
    "_, Xtest_tensor, _, Ytest_tensor = scale_and_convert_to_tensor(X_train, X_test, y_train, y_test, scaleTarget = True)\n",
    "\n",
    "print(\"X train tensor\",Xtrain_tensor.shape)\n",
    "print(\"Y train tensor\",Ytrain_tensor.shape)\n",
    "\n",
    "print(\"X val tensor\",Xval_tensor.shape)\n",
    "print(\"Y val tensor\",Yval_tensor.shape)\n",
    "\n",
    "print(\"X test tensor\",Xtest_tensor.shape)\n",
    "print(\"Y test tensor\",Ytest_tensor.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4. Define BNN training and evaluation pipeline**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model_and_evaluate_regression(Xtrain_tensor, Ytrain_tensor, Xtest_tensor, Ytest_tensor, layers = [32,8], learning_param = 0.01, kl_weight = 0.01, steps = 100, printStep = True):    \n",
    "    in_features = Xtrain_tensor.shape[1]\n",
    "    batch_size = Xtrain_tensor.shape[0]\n",
    "    \n",
    "    ## Ensure reproducibility\n",
    "    seed = 1\n",
    "    torch.manual_seed(seed)\n",
    "    \n",
    "    # Build model\n",
    "    layer = []\n",
    "    \n",
    "    ## Input layer\n",
    "    layer.append(bnn.BayesLinear(prior_mu=0, prior_sigma=0.1, in_features = in_features, out_features = layers[0]))\n",
    "    layer.append(nn.ReLU())\n",
    "    \n",
    "    ## Hidden layers\n",
    "    for index, neurons in enumerate(layers):\n",
    "        if index != (len(layers)-1):\n",
    "            layer.append(bnn.BayesLinear(prior_mu=0, prior_sigma=0.1, in_features=neurons, out_features=layers[index+1]))\n",
    "            layer.append(nn.ReLU())\n",
    "\n",
    "    ## Output layer\n",
    "    layer.append(bnn.BayesLinear(prior_mu=0, prior_sigma=0.1, in_features=layers[-1], out_features=1))\n",
    "    \n",
    "    model = nn.Sequential(*layer)\n",
    "    # Define Loss\n",
    "    mse_loss = nn.MSELoss()\n",
    "    kl_loss = bnn.BKLLoss(reduction='mean', last_layer_only=False)\n",
    "\n",
    "    ## Define optimiser with learning rate\n",
    "    optimizer = optim.Adam(model.parameters(), lr = learning_param)\n",
    "    \n",
    "    ### Train model\n",
    "    for step in range(steps):\n",
    "        torch.manual_seed(seed)\n",
    "        \n",
    "        pre = model(Xtrain_tensor)\n",
    "        mse = mse_loss(pre, Ytrain_tensor)\n",
    "        kl = kl_loss(model)\n",
    "        cost = mse + kl_weight*kl\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        cost.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Print Progress\n",
    "        if step%50==0 and printStep:\n",
    "            print('[Step %d]: MSE : %.8f, KL : %.8f' % (step , mse.item(), kl.item()))\n",
    "            \n",
    "    train_mse = mse.item()\n",
    "    kl_loss = kl.item()\n",
    "    \n",
    "    ## Predict Test\n",
    "    torch.manual_seed(seed)\n",
    "    y_predict = model(Xtest_tensor)\n",
    "    \n",
    "    ## Performance Evaluation on test - MSE\n",
    "    test_mse = mean_squared_error(Ytest_tensor.detach().numpy(),y_predict.detach().numpy())\n",
    "    \n",
    "    ## Inverse Standard Scaler - for unscaled RMSE \n",
    "    y_actual = scaler.inverse_transform(Ytest_tensor.detach().numpy().reshape(-1, 1))\n",
    "    y_predict = scaler.inverse_transform(y_predict.detach().numpy().reshape(-1, 1))\n",
    "    test_rmse = math.sqrt(mean_squared_error(y_actual,y_predict))\n",
    "    \n",
    "    return (model, y_predict, train_mse, test_mse, test_rmse, kl_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**6. Perform Grid Search**\n",
    "\n",
    "Perform grid search and evaluate based on validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Layer:  [32, 8]\n",
      "-- Learning Param:  0.001\n",
      "-- Learning Param:  0.011\n",
      "-- Learning Param:  0.021\n",
      "-- Learning Param:  0.032\n",
      "-- Learning Param:  0.042\n",
      "-- Learning Param:  0.052\n",
      "-- Learning Param:  0.062\n",
      "-- Learning Param:  0.072\n",
      "-- Learning Param:  0.082\n",
      "-- Learning Param:  0.093\n",
      "-- Learning Param:  0.103\n",
      "-- Learning Param:  0.113\n",
      "-- Learning Param:  0.123\n",
      "-- Learning Param:  0.133\n",
      "-- Learning Param:  0.144\n",
      "-- Learning Param:  0.154\n",
      "-- Learning Param:  0.164\n",
      "-- Learning Param:  0.174\n",
      "-- Learning Param:  0.184\n",
      "-- Learning Param:  0.194\n",
      "-- Learning Param:  0.205\n",
      "-- Learning Param:  0.215\n",
      "-- Learning Param:  0.225\n",
      "-- Learning Param:  0.235\n",
      "-- Learning Param:  0.245\n",
      "-- Learning Param:  0.256\n",
      "-- Learning Param:  0.266\n",
      "-- Learning Param:  0.276\n",
      "-- Learning Param:  0.286\n",
      "-- Learning Param:  0.296\n",
      "-- Learning Param:  0.307\n",
      "-- Learning Param:  0.317\n",
      "-- Learning Param:  0.327\n",
      "-- Learning Param:  0.337\n",
      "-- Learning Param:  0.347\n",
      "-- Learning Param:  0.357\n",
      "-- Learning Param:  0.368\n",
      "-- Learning Param:  0.378\n",
      "-- Learning Param:  0.388\n",
      "-- Learning Param:  0.398\n",
      "-- Learning Param:  0.408\n",
      "-- Learning Param:  0.419\n",
      "-- Learning Param:  0.429\n",
      "-- Learning Param:  0.439\n",
      "-- Learning Param:  0.449\n",
      "-- Learning Param:  0.459\n",
      "-- Learning Param:  0.469\n",
      "-- Learning Param:  0.48\n",
      "-- Learning Param:  0.49\n",
      "-- Learning Param:  0.5\n",
      "--- Layer:  [32, 16]\n",
      "-- Learning Param:  0.001\n",
      "-- Learning Param:  0.011\n",
      "-- Learning Param:  0.021\n",
      "-- Learning Param:  0.032\n",
      "-- Learning Param:  0.042\n",
      "-- Learning Param:  0.052\n",
      "-- Learning Param:  0.062\n",
      "-- Learning Param:  0.072\n",
      "-- Learning Param:  0.082\n",
      "-- Learning Param:  0.093\n",
      "-- Learning Param:  0.103\n",
      "-- Learning Param:  0.113\n",
      "-- Learning Param:  0.123\n",
      "-- Learning Param:  0.133\n",
      "-- Learning Param:  0.144\n",
      "-- Learning Param:  0.154\n",
      "-- Learning Param:  0.164\n",
      "-- Learning Param:  0.174\n",
      "-- Learning Param:  0.184\n",
      "-- Learning Param:  0.194\n",
      "-- Learning Param:  0.205\n",
      "-- Learning Param:  0.215\n",
      "-- Learning Param:  0.225\n",
      "-- Learning Param:  0.235\n",
      "-- Learning Param:  0.245\n",
      "-- Learning Param:  0.256\n",
      "-- Learning Param:  0.266\n",
      "-- Learning Param:  0.276\n",
      "-- Learning Param:  0.286\n",
      "-- Learning Param:  0.296\n",
      "-- Learning Param:  0.307\n",
      "-- Learning Param:  0.317\n",
      "-- Learning Param:  0.327\n",
      "-- Learning Param:  0.337\n",
      "-- Learning Param:  0.347\n",
      "-- Learning Param:  0.357\n",
      "-- Learning Param:  0.368\n",
      "-- Learning Param:  0.378\n",
      "-- Learning Param:  0.388\n",
      "-- Learning Param:  0.398\n",
      "-- Learning Param:  0.408\n",
      "-- Learning Param:  0.419\n",
      "-- Learning Param:  0.429\n",
      "-- Learning Param:  0.439\n",
      "-- Learning Param:  0.449\n",
      "-- Learning Param:  0.459\n",
      "-- Learning Param:  0.469\n",
      "-- Learning Param:  0.48\n",
      "-- Learning Param:  0.49\n",
      "-- Learning Param:  0.5\n",
      "--- Layer:  [32, 16, 8]\n",
      "-- Learning Param:  0.001\n",
      "-- Learning Param:  0.011\n",
      "-- Learning Param:  0.021\n",
      "-- Learning Param:  0.032\n",
      "-- Learning Param:  0.042\n",
      "-- Learning Param:  0.052\n",
      "-- Learning Param:  0.062\n",
      "-- Learning Param:  0.072\n",
      "-- Learning Param:  0.082\n",
      "-- Learning Param:  0.093\n",
      "-- Learning Param:  0.103\n",
      "-- Learning Param:  0.113\n",
      "-- Learning Param:  0.123\n",
      "-- Learning Param:  0.133\n",
      "-- Learning Param:  0.144\n",
      "-- Learning Param:  0.154\n",
      "-- Learning Param:  0.164\n",
      "-- Learning Param:  0.174\n",
      "-- Learning Param:  0.184\n",
      "-- Learning Param:  0.194\n",
      "-- Learning Param:  0.205\n",
      "-- Learning Param:  0.215\n",
      "-- Learning Param:  0.225\n",
      "-- Learning Param:  0.235\n",
      "-- Learning Param:  0.245\n",
      "-- Learning Param:  0.256\n",
      "-- Learning Param:  0.266\n",
      "-- Learning Param:  0.276\n",
      "-- Learning Param:  0.286\n",
      "-- Learning Param:  0.296\n",
      "-- Learning Param:  0.307\n",
      "-- Learning Param:  0.317\n",
      "-- Learning Param:  0.327\n",
      "-- Learning Param:  0.337\n",
      "-- Learning Param:  0.347\n",
      "-- Learning Param:  0.357\n",
      "-- Learning Param:  0.368\n",
      "-- Learning Param:  0.378\n",
      "-- Learning Param:  0.388\n",
      "-- Learning Param:  0.398\n",
      "-- Learning Param:  0.408\n",
      "-- Learning Param:  0.419\n",
      "-- Learning Param:  0.429\n",
      "-- Learning Param:  0.439\n",
      "-- Learning Param:  0.449\n",
      "-- Learning Param:  0.459\n",
      "-- Learning Param:  0.469\n",
      "-- Learning Param:  0.48\n",
      "-- Learning Param:  0.49\n",
      "-- Learning Param:  0.5\n",
      "Complete\n"
     ]
    }
   ],
   "source": [
    "training_mse_list = []\n",
    "val_mse_list = []\n",
    "val_rmse_list = []\n",
    "kl_list = []\n",
    "combination = []\n",
    "\n",
    "learning_param_list = pd.Series(np.linspace(0.001,0.5,50)).apply(lambda x: round(x,3))\n",
    "kl_weight =  pd.Series(np.linspace(0.001,0.5,50)).apply(lambda x: round(x,3))\n",
    "layers_list = ([32,8],[32,16],[32,16,8])\n",
    "\n",
    "for layer in layers_list:\n",
    "    print(\"--- Layer: \", layer)\n",
    "    for lr in learning_param_list:\n",
    "        print(\"-- Learning Param: \", lr)\n",
    "        for kl in kl_weight:\n",
    "            combination.append(\"layer: {} lr: {} kl: {}\".format(layer,lr,kl))\n",
    "            _ ,_, train_mse, val_mse, val_rmse, kl_loss = train_model_and_evaluate_regression(Xtrain_tensor, Ytrain_tensor, Xval_tensor, Yval_tensor, layer, learning_param = lr, kl_weight = kl, steps = 100, printStep = False)\n",
    "            training_mse_list.append(train_mse)\n",
    "            val_mse_list.append(val_mse)\n",
    "            val_rmse_list.append(val_rmse)\n",
    "            kl_list.append(kl_loss)\n",
    "            \n",
    "print(\"Complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer: [32, 16, 8] lr: 0.123 kl: 0.174\n"
     ]
    }
   ],
   "source": [
    "results = pd.DataFrame({\"Combination\": combination,\"Train MSE\":training_mse_list, \"Val MSE\":val_mse_list, \"Val RMSE\":  val_rmse_list,\"KL Loss\":kl_list})\n",
    "results.to_csv(\"Combinations_regression_withoutSentiments&lagged2.csv\")\n",
    "\n",
    "## Find the hyperparameters with gives the lowest test RMSE\n",
    "results[results['Val RMSE'] ==  results['Val RMSE'].min()]['Combination']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val RMSE:  1289.6513579258544\n"
     ]
    }
   ],
   "source": [
    "layer = [32, 16, 8]\n",
    "lr = 0.123\n",
    "kl = 0.154\n",
    "\n",
    "model, y_predict, _, _, test_rmse, kl_loss = train_model_and_evaluate_regression(Xtrain_tensor, Ytrain_tensor, Xval_tensor, Yval_tensor, layer, learning_param = lr, kl_weight = kl, steps = 100, printStep = False)\n",
    "print(\"Val RMSE: \",test_rmse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**7. Retrain the model with selected hyperparameters and all train data available**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Helper function for retraining the model with all data available\n",
    "def train_model_and_predict(Xtrain_tensor, Ytrain_tensor, layers = [32,8], learning_param = 0.01, kl_weight = 0.01, steps = 100):    \n",
    "    \"\"\" \n",
    "    Trains model and returns predictions on entire dataset.\n",
    "    \"\"\"\n",
    "    in_features = Xtrain_tensor.shape[1]\n",
    "    batch_size = Xtrain_tensor.shape[0]\n",
    "    \n",
    "    ## Ensure reproducibility\n",
    "    seed = 1\n",
    "    torch.manual_seed(seed)\n",
    "    \n",
    "    # Build model\n",
    "    layer = []\n",
    "    \n",
    "    ## Input layer\n",
    "    layer.append(bnn.BayesLinear(prior_mu=0, prior_sigma=0.1, in_features = in_features, out_features = layers[0]))\n",
    "    layer.append(nn.ReLU())\n",
    "    \n",
    "    ## Hidden layers\n",
    "    for index, neurons in enumerate(layers):\n",
    "        if index != (len(layers)-1):\n",
    "            layer.append(bnn.BayesLinear(prior_mu=0, prior_sigma=0.1, in_features=neurons, out_features=layers[index+1]))\n",
    "            layer.append(nn.ReLU())\n",
    "\n",
    "    ## Output layer\n",
    "    layer.append(bnn.BayesLinear(prior_mu=0, prior_sigma=0.1, in_features=layers[-1], out_features=1))\n",
    "    \n",
    "    model = nn.Sequential(*layer)\n",
    "    # Define Loss\n",
    "    mse_loss = nn.MSELoss()\n",
    "    kl_loss = bnn.BKLLoss(reduction='mean', last_layer_only=False)\n",
    "\n",
    "    ## Define optimiser with learning rate\n",
    "    optimizer = optim.Adam(model.parameters(), lr = learning_param)\n",
    "    \n",
    "    ### Train model\n",
    "    for step in range(steps):\n",
    "        torch.manual_seed(seed)\n",
    "        \n",
    "        pre = model(Xtrain_tensor)\n",
    "        mse = mse_loss(pre, Ytrain_tensor)\n",
    "        kl = kl_loss(model)\n",
    "        cost = mse + kl_weight*kl\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        cost.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "    ## Predict Test\n",
    "    torch.manual_seed(seed)\n",
    "    y_predict = model(Xtrain_tensor)\n",
    "\n",
    "    ## Inverse Standard Scaler\n",
    "    y_predict = scaler.inverse_transform(y_predict.detach().numpy().reshape(-1, 1))\n",
    "    \n",
    "    return (model,y_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X refit tensor torch.Size([88, 9])\n",
      "Y refit tensor torch.Size([88, 1])\n"
     ]
    }
   ],
   "source": [
    "### Standardise refit data (train + val)\n",
    "Xrefit_tensor, _, Yrefit_tensor, _ = scale_and_convert_to_tensor(X_refit, X_refit, y_refit, y_refit, scaleTarget = True)\n",
    "print(\"X refit tensor\",Xrefit_tensor.shape)\n",
    "print(\"Y refit tensor\",Yrefit_tensor.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test RMSE:  3140.482287802305\n",
      "[[55760.875]\n",
      " [55758.062]\n",
      " [55762.164]\n",
      " [55776.81 ]\n",
      " [55776.676]\n",
      " [55777.035]\n",
      " [55776.32 ]\n",
      " [55786.953]\n",
      " [55778.188]\n",
      " [55776.98 ]\n",
      " [55770.79 ]\n",
      " [55781.625]\n",
      " [55783.836]\n",
      " [55783.79 ]]\n"
     ]
    }
   ],
   "source": [
    "### Train on refit data and evaluate on test\n",
    "_, y_predict, _, _, test_rmse, kl_loss = train_model_and_evaluate_regression(Xrefit_tensor, Yrefit_tensor, Xtest_tensor, Ytest_tensor, layer, learning_param = lr, kl_weight = kl, steps = 100, printStep = False)\n",
    "print(\"Test RMSE: \",test_rmse)\n",
    "print(y_predict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**8. Obtain predictions for Backtesting**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X test tensor torch.Size([102, 9])\n",
      "Y test tensor torch.Size([102, 1])\n"
     ]
    }
   ],
   "source": [
    "Xfull_tensor, _, Yfull_tensor, _ = scale_and_convert_to_tensor(X_full, X_full, y_full, y_full, scaleTarget = True)\n",
    "print(\"X test tensor\",Xfull_tensor.shape)\n",
    "print(\"Y test tensor\",Yfull_tensor.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2323.6570432281137"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model, y_predict = train_model_and_predict(Xfull_tensor, Yfull_tensor, layers = layer, learning_param = lr, kl_weight = kl, steps = 100)        \n",
    "math.sqrt(mean_squared_error(y_full,y_predict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pd.DataFrame(y_predict).to_csv(\"out.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## With Sentiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1. Create Feature Lags**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_lags = {\"Adj_Close_BTC-USD\" : 1, \n",
    "                \"Adj_Close_SPY\" : 1,\n",
    "                \"Adj_Close_GLD\" : 1,\n",
    "                \"Adj_Close_CHFUSD=X\" : 1,\n",
    "                \"Adj_Close_EURUSD=X\" : 1,\n",
    "                \"Adj_Close_GBPUSD=X\" : 1,\n",
    "                \"Adj_Close_JPYUSD=X\" : 1,\n",
    "                \"blockchain_transactions_per_block\" : 1,\n",
    "                \"blockchain_hash_rates\" : 1,\n",
    "                \"coindesk_sentiment\" : 1,\n",
    "                \"reddit_comments_sentiments\" : 1,\n",
    "                \"top_50_reddit_posts_sentiments\" : 1}\n",
    "\n",
    "data = lag(df, feature_lags).drop_duplicates(subset = 'date')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2. Handle Train-test split**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data[(data[\"date\"] >= \"2021-01-01\") & (data[\"date\"] <= \"2021-04-12\")]\n",
    "\n",
    "## train, validation and test split\n",
    "train = data.loc[(data[\"date\"] >= \"2021-01-01\") & (data[\"date\"] <= \"2021-03-14\")]\n",
    "validation = data[(data[\"date\"] >= \"2021-03-15\") & (data[\"date\"] <= \"2021-03-29\")]\n",
    "test = data[(data[\"date\"] >= \"2021-03-30\") & (data[\"date\"] <= \"2021-04-12\")]\n",
    "\n",
    "## refit and full for later use \n",
    "refit = data[(data[\"date\"] >= \"2021-01-01\") & (data[\"date\"] <= \"2021-03-29\")]\n",
    "full = data.copy(deep = True)\n",
    "\n",
    "## train \n",
    "X_train = train.drop([\"date\", \"Adj_Close_BTC-USD\"], axis = 1)\n",
    "y_train = train[\"Adj_Close_BTC-USD\"]\n",
    "\n",
    "## validation\n",
    "X_val = validation.drop([\"date\", \"Adj_Close_BTC-USD\"], axis = 1)\n",
    "y_val = validation[\"Adj_Close_BTC-USD\"]\n",
    "\n",
    "## test\n",
    "X_test = test.drop([\"date\", \"Adj_Close_BTC-USD\"], axis = 1)\n",
    "y_test = test[\"Adj_Close_BTC-USD\"]\n",
    "\n",
    "## refit\n",
    "X_refit = refit.drop([\"date\", \"Adj_Close_BTC-USD\"], axis = 1)\n",
    "y_refit = refit[\"Adj_Close_BTC-USD\"]\n",
    "\n",
    "## full\n",
    "X_full = full.drop([\"date\", \"Adj_Close_BTC-USD\"], axis = 1)\n",
    "y_full = full[\"Adj_Close_BTC-USD\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3. Standardise dataset and transform to tensors** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X train tensor torch.Size([73, 12])\n",
      "Y train tensor torch.Size([73, 1])\n",
      "X val tensor torch.Size([15, 12])\n",
      "Y val tensor torch.Size([15, 1])\n",
      "X test tensor torch.Size([14, 12])\n",
      "Y test tensor torch.Size([14, 1])\n"
     ]
    }
   ],
   "source": [
    "## Standardise datasets and convert into tensors\n",
    "Xtrain_tensor, Xval_tensor, Ytrain_tensor, Yval_tensor = scale_and_convert_to_tensor(X_train, X_val, y_train, y_val, scaleTarget = True)\n",
    "_, Xtest_tensor, _, Ytest_tensor = scale_and_convert_to_tensor(X_train, X_test, y_train, y_test, scaleTarget = True)\n",
    "\n",
    "print(\"X train tensor\",Xtrain_tensor.shape)\n",
    "print(\"Y train tensor\",Ytrain_tensor.shape)\n",
    "\n",
    "print(\"X val tensor\",Xval_tensor.shape)\n",
    "print(\"Y val tensor\",Yval_tensor.shape)\n",
    "\n",
    "print(\"X test tensor\",Xtest_tensor.shape)\n",
    "print(\"Y test tensor\",Ytest_tensor.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4. Perform GridSearch**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Layer:  [32, 8]\n",
      "-- Learning Param:  0.001\n",
      "-- Learning Param:  0.011\n",
      "-- Learning Param:  0.021\n",
      "-- Learning Param:  0.032\n",
      "-- Learning Param:  0.042\n",
      "-- Learning Param:  0.052\n",
      "-- Learning Param:  0.062\n",
      "-- Learning Param:  0.072\n",
      "-- Learning Param:  0.082\n",
      "-- Learning Param:  0.093\n",
      "-- Learning Param:  0.103\n",
      "-- Learning Param:  0.113\n",
      "-- Learning Param:  0.123\n",
      "-- Learning Param:  0.133\n",
      "-- Learning Param:  0.144\n",
      "-- Learning Param:  0.154\n",
      "-- Learning Param:  0.164\n",
      "-- Learning Param:  0.174\n",
      "-- Learning Param:  0.184\n",
      "-- Learning Param:  0.194\n",
      "-- Learning Param:  0.205\n",
      "-- Learning Param:  0.215\n",
      "-- Learning Param:  0.225\n",
      "-- Learning Param:  0.235\n",
      "-- Learning Param:  0.245\n",
      "-- Learning Param:  0.256\n",
      "-- Learning Param:  0.266\n",
      "-- Learning Param:  0.276\n",
      "-- Learning Param:  0.286\n",
      "-- Learning Param:  0.296\n",
      "-- Learning Param:  0.307\n",
      "-- Learning Param:  0.317\n",
      "-- Learning Param:  0.327\n",
      "-- Learning Param:  0.337\n",
      "-- Learning Param:  0.347\n",
      "-- Learning Param:  0.357\n",
      "-- Learning Param:  0.368\n",
      "-- Learning Param:  0.378\n",
      "-- Learning Param:  0.388\n",
      "-- Learning Param:  0.398\n",
      "-- Learning Param:  0.408\n",
      "-- Learning Param:  0.419\n",
      "-- Learning Param:  0.429\n",
      "-- Learning Param:  0.439\n",
      "-- Learning Param:  0.449\n",
      "-- Learning Param:  0.459\n",
      "-- Learning Param:  0.469\n",
      "-- Learning Param:  0.48\n",
      "-- Learning Param:  0.49\n",
      "-- Learning Param:  0.5\n",
      "--- Layer:  [32, 16]\n",
      "-- Learning Param:  0.001\n",
      "-- Learning Param:  0.011\n",
      "-- Learning Param:  0.021\n",
      "-- Learning Param:  0.032\n",
      "-- Learning Param:  0.042\n",
      "-- Learning Param:  0.052\n",
      "-- Learning Param:  0.062\n",
      "-- Learning Param:  0.072\n",
      "-- Learning Param:  0.082\n",
      "-- Learning Param:  0.093\n",
      "-- Learning Param:  0.103\n",
      "-- Learning Param:  0.113\n",
      "-- Learning Param:  0.123\n",
      "-- Learning Param:  0.133\n",
      "-- Learning Param:  0.144\n",
      "-- Learning Param:  0.154\n",
      "-- Learning Param:  0.164\n",
      "-- Learning Param:  0.174\n",
      "-- Learning Param:  0.184\n",
      "-- Learning Param:  0.194\n",
      "-- Learning Param:  0.205\n",
      "-- Learning Param:  0.215\n",
      "-- Learning Param:  0.225\n",
      "-- Learning Param:  0.235\n",
      "-- Learning Param:  0.245\n",
      "-- Learning Param:  0.256\n",
      "-- Learning Param:  0.266\n",
      "-- Learning Param:  0.276\n",
      "-- Learning Param:  0.286\n",
      "-- Learning Param:  0.296\n",
      "-- Learning Param:  0.307\n",
      "-- Learning Param:  0.317\n",
      "-- Learning Param:  0.327\n",
      "-- Learning Param:  0.337\n",
      "-- Learning Param:  0.347\n",
      "-- Learning Param:  0.357\n",
      "-- Learning Param:  0.368\n",
      "-- Learning Param:  0.378\n",
      "-- Learning Param:  0.388\n",
      "-- Learning Param:  0.398\n",
      "-- Learning Param:  0.408\n",
      "-- Learning Param:  0.419\n",
      "-- Learning Param:  0.429\n",
      "-- Learning Param:  0.439\n",
      "-- Learning Param:  0.449\n",
      "-- Learning Param:  0.459\n",
      "-- Learning Param:  0.469\n",
      "-- Learning Param:  0.48\n",
      "-- Learning Param:  0.49\n",
      "-- Learning Param:  0.5\n",
      "--- Layer:  [32, 16, 8]\n",
      "-- Learning Param:  0.001\n",
      "-- Learning Param:  0.011\n",
      "-- Learning Param:  0.021\n",
      "-- Learning Param:  0.032\n",
      "-- Learning Param:  0.042\n",
      "-- Learning Param:  0.052\n",
      "-- Learning Param:  0.062\n",
      "-- Learning Param:  0.072\n",
      "-- Learning Param:  0.082\n",
      "-- Learning Param:  0.093\n",
      "-- Learning Param:  0.103\n",
      "-- Learning Param:  0.113\n",
      "-- Learning Param:  0.123\n",
      "-- Learning Param:  0.133\n",
      "-- Learning Param:  0.144\n",
      "-- Learning Param:  0.154\n",
      "-- Learning Param:  0.164\n",
      "-- Learning Param:  0.174\n",
      "-- Learning Param:  0.184\n",
      "-- Learning Param:  0.194\n",
      "-- Learning Param:  0.205\n",
      "-- Learning Param:  0.215\n",
      "-- Learning Param:  0.225\n",
      "-- Learning Param:  0.235\n",
      "-- Learning Param:  0.245\n",
      "-- Learning Param:  0.256\n",
      "-- Learning Param:  0.266\n",
      "-- Learning Param:  0.276\n",
      "-- Learning Param:  0.286\n",
      "-- Learning Param:  0.296\n",
      "-- Learning Param:  0.307\n",
      "-- Learning Param:  0.317\n",
      "-- Learning Param:  0.327\n",
      "-- Learning Param:  0.337\n",
      "-- Learning Param:  0.347\n",
      "-- Learning Param:  0.357\n",
      "-- Learning Param:  0.368\n",
      "-- Learning Param:  0.378\n",
      "-- Learning Param:  0.388\n",
      "-- Learning Param:  0.398\n",
      "-- Learning Param:  0.408\n",
      "-- Learning Param:  0.419\n",
      "-- Learning Param:  0.429\n",
      "-- Learning Param:  0.439\n",
      "-- Learning Param:  0.449\n",
      "-- Learning Param:  0.459\n",
      "-- Learning Param:  0.469\n",
      "-- Learning Param:  0.48\n",
      "-- Learning Param:  0.49\n",
      "-- Learning Param:  0.5\n",
      "Complete\n"
     ]
    }
   ],
   "source": [
    "training_mse_list = []\n",
    "val_mse_list = []\n",
    "val_rmse_list = []\n",
    "kl_list = []\n",
    "combination = []\n",
    "\n",
    "learning_param_list = pd.Series(np.linspace(0.001,0.5,50)).apply(lambda x: round(x,3))\n",
    "kl_weight =  pd.Series(np.linspace(0.001,0.5,50)).apply(lambda x: round(x,3))\n",
    "layers_list = ([32,8],[32,16],[32,16,8])\n",
    "\n",
    "for layer in layers_list:\n",
    "    print(\"--- Layer: \", layer)\n",
    "    for lr in learning_param_list:\n",
    "        print(\"-- Learning Param: \", lr)\n",
    "        for kl in kl_weight:\n",
    "            combination.append(\"layer: {} lr: {} kl: {}\".format(layer,lr,kl))\n",
    "            _ ,_, train_mse, val_mse, val_rmse, kl_loss = train_model_and_evaluate_regression(Xtrain_tensor, Ytrain_tensor, Xval_tensor, Yval_tensor, layer, learning_param = lr, kl_weight = kl, steps = 100, printStep = False)\n",
    "            training_mse_list.append(train_mse)\n",
    "            val_mse_list.append(val_mse)\n",
    "            val_rmse_list.append(val_rmse)\n",
    "            kl_list.append(kl_loss)\n",
    "            \n",
    "print(\"Complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4825    layer: [32, 16] lr: 0.469 kl: 0.256\n",
       "Name: Combination, dtype: object"
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = pd.DataFrame({\"Combination\": combination,\"Train MSE\":training_mse_list, \"Val MSE\":val_mse_list, \"Val RMSE\":  val_rmse_list,\"KL Loss\":kl_list})\n",
    "results.to_csv(\"Combinations_regression_withSentiments&lagged2.csv\")\n",
    "\n",
    "## Find the hyperparameters with gives the lowest test RMSE\n",
    "results[results['Val RMSE'] ==  results['Val RMSE'].min()]['Combination']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val RMSE:  1260.3213875833417\n"
     ]
    }
   ],
   "source": [
    "layer = [32, 16]\n",
    "lr =  0.469\n",
    "kl = 0.256\n",
    "\n",
    "model, y_predict, _, _, test_rmse, kl_loss = train_model_and_evaluate_regression(Xtrain_tensor, Ytrain_tensor, Xval_tensor, Yval_tensor, layer, learning_param = lr, kl_weight = kl, steps = 100, printStep = False)\n",
    "print(\"Val RMSE: \",test_rmse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**5. Retrain the model with selected hyperparameters and all data available.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X refit tensor torch.Size([88, 12])\n",
      "Y refit tensor torch.Size([88, 1])\n"
     ]
    }
   ],
   "source": [
    "### Standardise refit data (train + val)\n",
    "Xrefit_tensor, _, Yrefit_tensor, _ = scale_and_convert_to_tensor(X_refit, X_refit, y_refit, y_refit, scaleTarget = True)\n",
    "print(\"X refit tensor\",Xrefit_tensor.shape)\n",
    "print(\"Y refit tensor\",Yrefit_tensor.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test RMSE:  7158.665797479304\n",
      "[[47359.727]\n",
      " [47295.473]\n",
      " [53280.14 ]\n",
      " [47736.695]\n",
      " [48592.508]\n",
      " [53786.812]\n",
      " [47595.332]\n",
      " [58345.59 ]\n",
      " [60096.895]\n",
      " [57160.02 ]\n",
      " [54645.5  ]\n",
      " [60626.598]\n",
      " [61218.37 ]\n",
      " [56808.38 ]]\n"
     ]
    }
   ],
   "source": [
    "### Train on refit data and evaluate on test\n",
    "_, y_predict, _, _, test_rmse, kl_loss = train_model_and_evaluate_regression(Xrefit_tensor, Yrefit_tensor, Xtest_tensor, Ytest_tensor, layers = layer, learning_param = lr, kl_weight = kl, steps = 100, printStep = False)\n",
    "print(\"Test RMSE: \", test_rmse)\n",
    "print(y_predict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**6. Obtain predictions for backtest**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X test tensor torch.Size([102, 12])\n",
      "Y test tensor torch.Size([102, 1])\n"
     ]
    }
   ],
   "source": [
    "Xfull_tensor, _, Yfull_tensor, _ = scale_and_convert_to_tensor(X_full, X_full, y_full, y_full, scaleTarget = True)\n",
    "print(\"X test tensor\",Xfull_tensor.shape)\n",
    "print(\"Y test tensor\",Yfull_tensor.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9967.97182503028"
      ]
     },
     "execution_count": 223,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model, y_predict = train_model_and_predict(Xfull_tensor, Yfull_tensor, layers = layer, learning_param = lr, kl_weight = kl, steps = 100)        \n",
    "math.sqrt(mean_squared_error(y_full,y_predict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pd.DataFrame(y_predict).to_csv(\"out.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
