{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Bitcoin price prediction with Bayesian Neural Network Regression using torchBNN and PyTorch.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "import datetime\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torchinfo import summary\n",
    "import torchbnn as bnn\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import r2_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "cwd = os.getcwd()\n",
    "from os.path import dirname, abspath\n",
    "while not cwd.endswith('BT4222_repo'):\n",
    "    cwd = os.path.dirname(cwd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = os.path.join(cwd,'data','cooked_data','cooked_complete_dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>Adj_Close_BTC-USD</th>\n",
       "      <th>Open_BTC-USD</th>\n",
       "      <th>High_BTC-USD</th>\n",
       "      <th>Low_BTC-USD</th>\n",
       "      <th>Volume_BTC-USD</th>\n",
       "      <th>Adj_Close_SPY</th>\n",
       "      <th>Adj_Close_GLD</th>\n",
       "      <th>Adj_Close_CHFUSD=X</th>\n",
       "      <th>Adj_Close_CNYUSD=X</th>\n",
       "      <th>Adj_Close_EURUSD=X</th>\n",
       "      <th>Adj_Close_GBPUSD=X</th>\n",
       "      <th>Adj_Close_JPYUSD=X</th>\n",
       "      <th>coindesk_sentiment</th>\n",
       "      <th>num_of_coindesk_posts</th>\n",
       "      <th>reddit_comments_sentiments</th>\n",
       "      <th>top_50_reddit_posts_sentiments</th>\n",
       "      <th>blockchain_transactions_per_block</th>\n",
       "      <th>blockchain_hash_rates</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>14/12/20</td>\n",
       "      <td>19246.6445</td>\n",
       "      <td>19144.4922</td>\n",
       "      <td>19305.0996</td>\n",
       "      <td>19012.7090</td>\n",
       "      <td>2.250000e+10</td>\n",
       "      <td>361.926788</td>\n",
       "      <td>171.539993</td>\n",
       "      <td>1.125442</td>\n",
       "      <td>0.152772</td>\n",
       "      <td>1.213340</td>\n",
       "      <td>1.331824</td>\n",
       "      <td>0.009621</td>\n",
       "      <td>0.249489</td>\n",
       "      <td>12</td>\n",
       "      <td>0.158060</td>\n",
       "      <td>0.677618</td>\n",
       "      <td>2167.93103</td>\n",
       "      <td>134533588</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>15/12/20</td>\n",
       "      <td>19417.0762</td>\n",
       "      <td>19246.9199</td>\n",
       "      <td>19525.0078</td>\n",
       "      <td>19079.8418</td>\n",
       "      <td>2.670000e+10</td>\n",
       "      <td>366.819824</td>\n",
       "      <td>173.940002</td>\n",
       "      <td>1.127930</td>\n",
       "      <td>0.152679</td>\n",
       "      <td>1.214890</td>\n",
       "      <td>1.333084</td>\n",
       "      <td>0.009614</td>\n",
       "      <td>0.173773</td>\n",
       "      <td>18</td>\n",
       "      <td>0.101930</td>\n",
       "      <td>0.447277</td>\n",
       "      <td>2288.85714</td>\n",
       "      <td>133351912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>16/12/20</td>\n",
       "      <td>21310.5977</td>\n",
       "      <td>19418.8184</td>\n",
       "      <td>21458.9082</td>\n",
       "      <td>19298.3164</td>\n",
       "      <td>4.440000e+10</td>\n",
       "      <td>367.395508</td>\n",
       "      <td>174.899994</td>\n",
       "      <td>1.129382</td>\n",
       "      <td>0.152945</td>\n",
       "      <td>1.215430</td>\n",
       "      <td>1.344447</td>\n",
       "      <td>0.009649</td>\n",
       "      <td>0.341491</td>\n",
       "      <td>11</td>\n",
       "      <td>0.127344</td>\n",
       "      <td>0.480809</td>\n",
       "      <td>2204.31469</td>\n",
       "      <td>132323572</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>17/12/20</td>\n",
       "      <td>22805.1621</td>\n",
       "      <td>21308.3516</td>\n",
       "      <td>23642.6602</td>\n",
       "      <td>21234.6758</td>\n",
       "      <td>7.140000e+10</td>\n",
       "      <td>369.449982</td>\n",
       "      <td>176.740006</td>\n",
       "      <td>1.129446</td>\n",
       "      <td>0.153109</td>\n",
       "      <td>1.219959</td>\n",
       "      <td>1.350293</td>\n",
       "      <td>0.009664</td>\n",
       "      <td>0.197572</td>\n",
       "      <td>10</td>\n",
       "      <td>0.135945</td>\n",
       "      <td>0.539729</td>\n",
       "      <td>2399.07752</td>\n",
       "      <td>132373209</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>18/12/20</td>\n",
       "      <td>23137.9609</td>\n",
       "      <td>22806.7969</td>\n",
       "      <td>23238.6016</td>\n",
       "      <td>22399.8125</td>\n",
       "      <td>4.040000e+10</td>\n",
       "      <td>367.974793</td>\n",
       "      <td>176.440002</td>\n",
       "      <td>1.130301</td>\n",
       "      <td>0.153090</td>\n",
       "      <td>1.226272</td>\n",
       "      <td>1.357018</td>\n",
       "      <td>0.009696</td>\n",
       "      <td>0.315601</td>\n",
       "      <td>2</td>\n",
       "      <td>0.135441</td>\n",
       "      <td>0.449503</td>\n",
       "      <td>2392.03185</td>\n",
       "      <td>131791042</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       date  Adj_Close_BTC-USD  Open_BTC-USD  High_BTC-USD  Low_BTC-USD  \\\n",
       "0  14/12/20         19246.6445    19144.4922    19305.0996   19012.7090   \n",
       "1  15/12/20         19417.0762    19246.9199    19525.0078   19079.8418   \n",
       "2  16/12/20         21310.5977    19418.8184    21458.9082   19298.3164   \n",
       "3  17/12/20         22805.1621    21308.3516    23642.6602   21234.6758   \n",
       "4  18/12/20         23137.9609    22806.7969    23238.6016   22399.8125   \n",
       "\n",
       "   Volume_BTC-USD  Adj_Close_SPY  Adj_Close_GLD  Adj_Close_CHFUSD=X  \\\n",
       "0    2.250000e+10     361.926788     171.539993            1.125442   \n",
       "1    2.670000e+10     366.819824     173.940002            1.127930   \n",
       "2    4.440000e+10     367.395508     174.899994            1.129382   \n",
       "3    7.140000e+10     369.449982     176.740006            1.129446   \n",
       "4    4.040000e+10     367.974793     176.440002            1.130301   \n",
       "\n",
       "   Adj_Close_CNYUSD=X  Adj_Close_EURUSD=X  Adj_Close_GBPUSD=X  \\\n",
       "0            0.152772            1.213340            1.331824   \n",
       "1            0.152679            1.214890            1.333084   \n",
       "2            0.152945            1.215430            1.344447   \n",
       "3            0.153109            1.219959            1.350293   \n",
       "4            0.153090            1.226272            1.357018   \n",
       "\n",
       "   Adj_Close_JPYUSD=X  coindesk_sentiment  num_of_coindesk_posts  \\\n",
       "0            0.009621            0.249489                     12   \n",
       "1            0.009614            0.173773                     18   \n",
       "2            0.009649            0.341491                     11   \n",
       "3            0.009664            0.197572                     10   \n",
       "4            0.009696            0.315601                      2   \n",
       "\n",
       "   reddit_comments_sentiments  top_50_reddit_posts_sentiments  \\\n",
       "0                    0.158060                        0.677618   \n",
       "1                    0.101930                        0.447277   \n",
       "2                    0.127344                        0.480809   \n",
       "3                    0.135945                        0.539729   \n",
       "4                    0.135441                        0.449503   \n",
       "\n",
       "   blockchain_transactions_per_block  blockchain_hash_rates  \n",
       "0                         2167.93103              134533588  \n",
       "1                         2288.85714              133351912  \n",
       "2                         2204.31469              132323572  \n",
       "3                         2399.07752              132373209  \n",
       "4                         2392.03185              131791042  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(PATH)\n",
    "df.dropna(inplace = True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['date'] = df['date'].apply(lambda x: datetime.datetime.strptime(x, \"%d/%m/%y\"))\n",
    "df[\"date\"] = pd.to_datetime(df[\"date\"], format='%d/%m/%Y', infer_datetime_format=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Helper Functions for creating lags and scaling**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lag(data, dic):\n",
    "    cols = []\n",
    "    for key, value in dic.items():\n",
    "        for i in range(1, value+1):\n",
    "            cols.append(data[key].shift(i).rename('{}_lag{}'.format(data[key].name, i)))\n",
    "    return pd.concat([data[\"date\"],data[\"Adj_Close_BTC-USD\"]] + cols, axis = 1)\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "def scale_and_convert_to_tensor(Xtrain, Xtest, Ytrain, Ytest, scaleTarget = False):  \n",
    "    global scaler\n",
    "    \n",
    "    # Standardise features\n",
    "    Xtrain_standardised = scaler.fit_transform(Xtrain)\n",
    "    Xtest_standardised = scaler.transform(Xtest)\n",
    "    \n",
    "    # Standardise target\n",
    "    Ytrain_standardised = Ytrain\n",
    "    Ytest_standardised = Ytest\n",
    "    \n",
    "    if scaleTarget:    \n",
    "        Ytrain_standardised = scaler.fit_transform(np.array(Ytrain).reshape(-1, 1))\n",
    "        Ytest_standardised = scaler.transform(np.array(Ytest).reshape(-1, 1))\n",
    "    \n",
    "    ## Change to tensor\n",
    "    Xtrain_tensor = torch.from_numpy(Xtrain_standardised).float()\n",
    "    Ytrain_tensor = torch.from_numpy(np.array(Ytrain_standardised)).float()\n",
    "    Xtest_tensor = torch.from_numpy(Xtest_standardised).float()\n",
    "    Ytest_tensor = torch.from_numpy(np.array(Ytest_standardised)).float()\n",
    "        \n",
    "    return (Xtrain_tensor, Xtest_tensor, Ytrain_tensor, Ytest_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Without Sentiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1. Create feature lags**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_lags = {\"Adj_Close_BTC-USD\" : 1,\n",
    "                \"Adj_Close_SPY\" : 1,\n",
    "                \"Adj_Close_GLD\" : 1,\n",
    "                \"Adj_Close_CHFUSD=X\" : 1,\n",
    "                \"Adj_Close_EURUSD=X\" : 1,\n",
    "                \"Adj_Close_GBPUSD=X\" : 1,\n",
    "                \"Adj_Close_JPYUSD=X\" : 1,\n",
    "                \"blockchain_transactions_per_block\" : 1,\n",
    "                \"blockchain_hash_rates\" : 1}\n",
    "\n",
    "data = lag(df, feature_lags)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2. Handle train-test split**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data[(data[\"date\"] >= \"2021-01-01\") & (data[\"date\"] <= \"2021-04-12\")]\n",
    "\n",
    "## train, validation and test split\n",
    "train = data.loc[(data[\"date\"] >= \"2021-01-01\") & (data[\"date\"] <= \"2021-03-14\")]\n",
    "validation = data[(data[\"date\"] >= \"2021-03-15\") & (data[\"date\"] <= \"2021-03-29\")]\n",
    "test = data[(data[\"date\"] >= \"2021-03-30\") & (data[\"date\"] <= \"2021-04-12\")]\n",
    "\n",
    "## refit and full for later use \n",
    "refit = data[(data[\"date\"] >= \"2021-01-01\") & (data[\"date\"] <= \"2021-03-29\")]\n",
    "full = data.copy(deep = True)\n",
    "\n",
    "## train \n",
    "X_train = train.drop([\"date\", \"Adj_Close_BTC-USD\"], axis = 1)\n",
    "y_train = train[\"Adj_Close_BTC-USD\"]\n",
    "\n",
    "## validation\n",
    "X_val = validation.drop([\"date\", \"Adj_Close_BTC-USD\"], axis = 1)\n",
    "y_val = validation[\"Adj_Close_BTC-USD\"]\n",
    "\n",
    "## test\n",
    "X_test = test.drop([\"date\", \"Adj_Close_BTC-USD\"], axis = 1)\n",
    "y_test = test[\"Adj_Close_BTC-USD\"]\n",
    "\n",
    "## refit\n",
    "X_refit = refit.drop([\"date\", \"Adj_Close_BTC-USD\"], axis = 1)\n",
    "y_refit = refit[\"Adj_Close_BTC-USD\"]\n",
    "\n",
    "## full\n",
    "X_full = full.drop([\"date\", \"Adj_Close_BTC-USD\"], axis = 1)\n",
    "y_full = full[\"Adj_Close_BTC-USD\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3. Standardise dataset and transform into tensors for pytorch**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X train tensor torch.Size([73, 9])\n",
      "Y train tensor torch.Size([73, 1])\n",
      "X val tensor torch.Size([15, 9])\n",
      "Y val tensor torch.Size([15, 1])\n",
      "X test tensor torch.Size([14, 9])\n",
      "Y test tensor torch.Size([14, 1])\n"
     ]
    }
   ],
   "source": [
    "## Standardise datasets and convert into tensors\n",
    "Xtrain_tensor, Xval_tensor, Ytrain_tensor, Yval_tensor = scale_and_convert_to_tensor(X_train, X_val, y_train, y_val, scaleTarget = True)\n",
    "_, Xtest_tensor, _, Ytest_tensor = scale_and_convert_to_tensor(X_train, X_test, y_train, y_test, scaleTarget = True)\n",
    "\n",
    "print(\"X train tensor\",Xtrain_tensor.shape)\n",
    "print(\"Y train tensor\",Ytrain_tensor.shape)\n",
    "\n",
    "print(\"X val tensor\",Xval_tensor.shape)\n",
    "print(\"Y val tensor\",Yval_tensor.shape)\n",
    "\n",
    "print(\"X test tensor\",Xtest_tensor.shape)\n",
    "print(\"Y test tensor\",Ytest_tensor.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4. Define BNN training and evaluation pipeline**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model_and_evaluate_regression(Xtrain_tensor, Ytrain_tensor, Xtest_tensor, Ytest_tensor, layers = [32,8], learning_param = 0.01, kl_weight = 0.01, steps = 100, printStep = True):    \n",
    "    in_features = Xtrain_tensor.shape[1]\n",
    "    batch_size = Xtrain_tensor.shape[0]\n",
    "    \n",
    "    ## Ensure reproducibility\n",
    "    seed = 1\n",
    "    torch.manual_seed(seed)\n",
    "    \n",
    "    # Build model\n",
    "    layer = []\n",
    "    \n",
    "    ## Input layer\n",
    "    layer.append(bnn.BayesLinear(prior_mu=0, prior_sigma=0.1, in_features = in_features, out_features = layers[0]))\n",
    "    layer.append(nn.ReLU())\n",
    "    \n",
    "    ## Hidden layers\n",
    "    for index, neurons in enumerate(layers):\n",
    "        if index != (len(layers)-1):\n",
    "            layer.append(bnn.BayesLinear(prior_mu=0, prior_sigma=0.1, in_features=neurons, out_features=layers[index+1]))\n",
    "            layer.append(nn.ReLU())\n",
    "\n",
    "    ## Output layer\n",
    "    layer.append(bnn.BayesLinear(prior_mu=0, prior_sigma=0.1, in_features=layers[-1], out_features=1))\n",
    "    \n",
    "    model = nn.Sequential(*layer)\n",
    "    # Define Loss\n",
    "    mse_loss = nn.MSELoss()\n",
    "    kl_loss = bnn.BKLLoss(reduction='mean', last_layer_only=False)\n",
    "\n",
    "    ## Define optimiser with learning rate\n",
    "    optimizer = optim.Adam(model.parameters(), lr = learning_param)\n",
    "    \n",
    "    ### Train model\n",
    "    for step in range(steps):\n",
    "        torch.manual_seed(seed)\n",
    "        \n",
    "        pre = model(Xtrain_tensor)\n",
    "        mse = mse_loss(pre, Ytrain_tensor)\n",
    "        kl = kl_loss(model)\n",
    "        cost = mse + kl_weight*kl\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        cost.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Print Progress\n",
    "        if step%50==0 and printStep:\n",
    "            print('[Step %d]: MSE : %.8f, KL : %.8f' % (step , mse.item(), kl.item()))\n",
    "            \n",
    "    train_mse = mse.item()\n",
    "    kl_loss = kl.item()\n",
    "    \n",
    "    ## Predict Test\n",
    "    torch.manual_seed(seed)\n",
    "    y_predict = model(Xtest_tensor)\n",
    "    \n",
    "    ## Performance Evaluation on test - MSE\n",
    "    test_mse = mean_squared_error(Ytest_tensor.detach().numpy(),y_predict.detach().numpy())\n",
    "    \n",
    "    ## Inverse Standard Scaler - for unscaled RMSE \n",
    "    y_actual = scaler.inverse_transform(Ytest_tensor.detach().numpy().reshape(-1, 1))\n",
    "    y_predict = scaler.inverse_transform(y_predict.detach().numpy().reshape(-1, 1))\n",
    "    test_rmse = math.sqrt(mean_squared_error(y_actual,y_predict))\n",
    "    \n",
    "    return (model, y_predict, train_mse, test_mse, test_rmse, kl_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**6. Perform Grid Search**\n",
    "\n",
    "Perform grid search and evaluate based on validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Layer:  [32, 8]\n",
      "-- Learning Param:  0.001\n",
      "-- Learning Param:  0.011\n",
      "-- Learning Param:  0.021\n",
      "-- Learning Param:  0.032\n",
      "-- Learning Param:  0.042\n",
      "-- Learning Param:  0.052\n",
      "-- Learning Param:  0.062\n",
      "-- Learning Param:  0.072\n",
      "-- Learning Param:  0.082\n",
      "-- Learning Param:  0.093\n",
      "-- Learning Param:  0.103\n",
      "-- Learning Param:  0.113\n",
      "-- Learning Param:  0.123\n",
      "-- Learning Param:  0.133\n",
      "-- Learning Param:  0.144\n",
      "-- Learning Param:  0.154\n",
      "-- Learning Param:  0.164\n",
      "-- Learning Param:  0.174\n",
      "-- Learning Param:  0.184\n",
      "-- Learning Param:  0.194\n",
      "-- Learning Param:  0.205\n",
      "-- Learning Param:  0.215\n",
      "-- Learning Param:  0.225\n",
      "-- Learning Param:  0.235\n",
      "-- Learning Param:  0.245\n",
      "-- Learning Param:  0.256\n",
      "-- Learning Param:  0.266\n",
      "-- Learning Param:  0.276\n",
      "-- Learning Param:  0.286\n",
      "-- Learning Param:  0.296\n",
      "-- Learning Param:  0.307\n",
      "-- Learning Param:  0.317\n",
      "-- Learning Param:  0.327\n",
      "-- Learning Param:  0.337\n",
      "-- Learning Param:  0.347\n",
      "-- Learning Param:  0.357\n",
      "-- Learning Param:  0.368\n",
      "-- Learning Param:  0.378\n",
      "-- Learning Param:  0.388\n",
      "-- Learning Param:  0.398\n",
      "-- Learning Param:  0.408\n",
      "-- Learning Param:  0.419\n",
      "-- Learning Param:  0.429\n",
      "-- Learning Param:  0.439\n",
      "-- Learning Param:  0.449\n",
      "-- Learning Param:  0.459\n",
      "-- Learning Param:  0.469\n",
      "-- Learning Param:  0.48\n",
      "-- Learning Param:  0.49\n",
      "-- Learning Param:  0.5\n",
      "--- Layer:  [32, 16]\n",
      "-- Learning Param:  0.001\n",
      "-- Learning Param:  0.011\n",
      "-- Learning Param:  0.021\n",
      "-- Learning Param:  0.032\n",
      "-- Learning Param:  0.042\n",
      "-- Learning Param:  0.052\n",
      "-- Learning Param:  0.062\n",
      "-- Learning Param:  0.072\n",
      "-- Learning Param:  0.082\n",
      "-- Learning Param:  0.093\n",
      "-- Learning Param:  0.103\n",
      "-- Learning Param:  0.113\n",
      "-- Learning Param:  0.123\n",
      "-- Learning Param:  0.133\n",
      "-- Learning Param:  0.144\n",
      "-- Learning Param:  0.154\n",
      "-- Learning Param:  0.164\n",
      "-- Learning Param:  0.174\n",
      "-- Learning Param:  0.184\n",
      "-- Learning Param:  0.194\n",
      "-- Learning Param:  0.205\n",
      "-- Learning Param:  0.215\n",
      "-- Learning Param:  0.225\n",
      "-- Learning Param:  0.235\n",
      "-- Learning Param:  0.245\n",
      "-- Learning Param:  0.256\n",
      "-- Learning Param:  0.266\n",
      "-- Learning Param:  0.276\n",
      "-- Learning Param:  0.286\n",
      "-- Learning Param:  0.296\n",
      "-- Learning Param:  0.307\n",
      "-- Learning Param:  0.317\n",
      "-- Learning Param:  0.327\n",
      "-- Learning Param:  0.337\n",
      "-- Learning Param:  0.347\n",
      "-- Learning Param:  0.357\n",
      "-- Learning Param:  0.368\n",
      "-- Learning Param:  0.378\n",
      "-- Learning Param:  0.388\n",
      "-- Learning Param:  0.398\n",
      "-- Learning Param:  0.408\n",
      "-- Learning Param:  0.419\n",
      "-- Learning Param:  0.429\n",
      "-- Learning Param:  0.439\n",
      "-- Learning Param:  0.449\n",
      "-- Learning Param:  0.459\n",
      "-- Learning Param:  0.469\n",
      "-- Learning Param:  0.48\n",
      "-- Learning Param:  0.49\n",
      "-- Learning Param:  0.5\n",
      "--- Layer:  [32, 16, 8]\n",
      "-- Learning Param:  0.001\n",
      "-- Learning Param:  0.011\n",
      "-- Learning Param:  0.021\n",
      "-- Learning Param:  0.032\n",
      "-- Learning Param:  0.042\n",
      "-- Learning Param:  0.052\n",
      "-- Learning Param:  0.062\n",
      "-- Learning Param:  0.072\n",
      "-- Learning Param:  0.082\n",
      "-- Learning Param:  0.093\n",
      "-- Learning Param:  0.103\n",
      "-- Learning Param:  0.113\n",
      "-- Learning Param:  0.123\n",
      "-- Learning Param:  0.133\n",
      "-- Learning Param:  0.144\n",
      "-- Learning Param:  0.154\n",
      "-- Learning Param:  0.164\n",
      "-- Learning Param:  0.174\n",
      "-- Learning Param:  0.184\n",
      "-- Learning Param:  0.194\n",
      "-- Learning Param:  0.205\n",
      "-- Learning Param:  0.215\n",
      "-- Learning Param:  0.225\n",
      "-- Learning Param:  0.235\n",
      "-- Learning Param:  0.245\n",
      "-- Learning Param:  0.256\n",
      "-- Learning Param:  0.266\n",
      "-- Learning Param:  0.276\n",
      "-- Learning Param:  0.286\n",
      "-- Learning Param:  0.296\n",
      "-- Learning Param:  0.307\n",
      "-- Learning Param:  0.317\n",
      "-- Learning Param:  0.327\n",
      "-- Learning Param:  0.337\n",
      "-- Learning Param:  0.347\n",
      "-- Learning Param:  0.357\n",
      "-- Learning Param:  0.368\n",
      "-- Learning Param:  0.378\n",
      "-- Learning Param:  0.388\n",
      "-- Learning Param:  0.398\n",
      "-- Learning Param:  0.408\n",
      "-- Learning Param:  0.419\n",
      "-- Learning Param:  0.429\n",
      "-- Learning Param:  0.439\n",
      "-- Learning Param:  0.449\n",
      "-- Learning Param:  0.459\n",
      "-- Learning Param:  0.469\n",
      "-- Learning Param:  0.48\n",
      "-- Learning Param:  0.49\n",
      "-- Learning Param:  0.5\n",
      "Complete\n"
     ]
    }
   ],
   "source": [
    "training_mse_list = []\n",
    "val_mse_list = []\n",
    "val_rmse_list = []\n",
    "kl_list = []\n",
    "combination = []\n",
    "\n",
    "learning_param_list = pd.Series(np.linspace(0.001,0.5,50)).apply(lambda x: round(x,3))\n",
    "kl_weight =  pd.Series(np.linspace(0.001,0.5,50)).apply(lambda x: round(x,3))\n",
    "layers_list = ([32,8],[32,16],[32,16,8])\n",
    "\n",
    "for layer in layers_list:\n",
    "    print(\"--- Layer: \", layer)\n",
    "    for lr in learning_param_list:\n",
    "        print(\"-- Learning Param: \", lr)\n",
    "        for kl in kl_weight:\n",
    "            combination.append(\"layer: {} lr: {} kl: {}\".format(layer,lr,kl))\n",
    "            _ ,_, train_mse, val_mse, val_rmse, kl_loss = train_model_and_evaluate_regression(Xtrain_tensor, Ytrain_tensor, Xval_tensor, Yval_tensor, layer, learning_param = lr, kl_weight = kl, steps = 100, printStep = False)\n",
    "            training_mse_list.append(train_mse)\n",
    "            val_mse_list.append(val_mse)\n",
    "            val_rmse_list.append(val_rmse)\n",
    "            kl_list.append(kl_loss)\n",
    "            \n",
    "print(\"Complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6588    layer: [32, 16, 8] lr: 0.317 kl: 0.388\n",
       "Name: Combination, dtype: object"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = pd.DataFrame({\"Combination\": combination,\"Train MSE\":training_mse_list, \"Val MSE\":val_mse_list, \"Val RMSE\":  val_rmse_list,\"KL Loss\":kl_list})\n",
    "results.to_csv(\"Combinations_regression_withoutSentiments&lagged2.csv\")\n",
    "\n",
    "## Find the hyperparameters with gives the lowest test RMSE\n",
    "results[results['Val RMSE'] ==  results['Val RMSE'].min()]['Combination']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val RMSE:  1349.3569301708128\n"
     ]
    }
   ],
   "source": [
    "layer = [32, 16, 8]\n",
    "lr = 0.317\n",
    "kl = 0.388\n",
    "\n",
    "model, y_predict, _, _, test_rmse, kl_loss = train_model_and_evaluate_regression(Xtrain_tensor, Ytrain_tensor, Xval_tensor, Yval_tensor, layer, learning_param = lr, kl_weight = kl, steps = 100, printStep = False)\n",
    "print(\"Val RMSE: \",test_rmse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**7. Retrain the model with selected hyperparameters and all train data available**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Helper function for retraining the model with all data available\n",
    "def train_model_and_predict(Xtrain_tensor, Ytrain_tensor, layers = [32,8], learning_param = 0.01, kl_weight = 0.01, steps = 100):    \n",
    "    \"\"\" \n",
    "    Trains model and returns predictions on entire dataset.\n",
    "    \"\"\"\n",
    "    in_features = Xtrain_tensor.shape[1]\n",
    "    batch_size = Xtrain_tensor.shape[0]\n",
    "    \n",
    "    ## Ensure reproducibility\n",
    "    seed = 1\n",
    "    torch.manual_seed(seed)\n",
    "    \n",
    "    # Build model\n",
    "    layer = []\n",
    "    \n",
    "    ## Input layer\n",
    "    layer.append(bnn.BayesLinear(prior_mu=0, prior_sigma=0.1, in_features = in_features, out_features = layers[0]))\n",
    "    layer.append(nn.ReLU())\n",
    "    \n",
    "    ## Hidden layers\n",
    "    for index, neurons in enumerate(layers):\n",
    "        if index != (len(layers)-1):\n",
    "            layer.append(bnn.BayesLinear(prior_mu=0, prior_sigma=0.1, in_features=neurons, out_features=layers[index+1]))\n",
    "            layer.append(nn.ReLU())\n",
    "\n",
    "    ## Output layer\n",
    "    layer.append(bnn.BayesLinear(prior_mu=0, prior_sigma=0.1, in_features=layers[-1], out_features=1))\n",
    "    \n",
    "    model = nn.Sequential(*layer)\n",
    "    # Define Loss\n",
    "    mse_loss = nn.MSELoss()\n",
    "    kl_loss = bnn.BKLLoss(reduction='mean', last_layer_only=False)\n",
    "\n",
    "    ## Define optimiser with learning rate\n",
    "    optimizer = optim.Adam(model.parameters(), lr = learning_param)\n",
    "    \n",
    "    ### Train model\n",
    "    for step in range(steps):\n",
    "        torch.manual_seed(seed)\n",
    "        \n",
    "        pre = model(Xtrain_tensor)\n",
    "        mse = mse_loss(pre, Ytrain_tensor)\n",
    "        kl = kl_loss(model)\n",
    "        cost = mse + kl_weight*kl\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        cost.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "    ## Predict Test\n",
    "    torch.manual_seed(seed)\n",
    "    y_predict = model(Xtrain_tensor)\n",
    "\n",
    "    ## Inverse Standard Scaler\n",
    "    y_predict = scaler.inverse_transform(y_predict.detach().numpy().reshape(-1, 1))\n",
    "    \n",
    "    return (model,y_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X refit tensor torch.Size([88, 9])\n",
      "Y refit tensor torch.Size([88, 1])\n"
     ]
    }
   ],
   "source": [
    "### Standardise refit data (train + val)\n",
    "Xrefit_tensor, _, Yrefit_tensor, _ = scale_and_convert_to_tensor(X_refit, X_refit, y_refit, y_refit, scaleTarget = True)\n",
    "print(\"X refit tensor\",Xrefit_tensor.shape)\n",
    "print(\"Y refit tensor\",Yrefit_tensor.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test RMSE:  2908.2783566914636\n",
      "[[56752.625]\n",
      " [56651.863]\n",
      " [56797.46 ]\n",
      " [56777.953]\n",
      " [56573.12 ]\n",
      " [56730.082]\n",
      " [56963.027]\n",
      " [56378.035]\n",
      " [55958.414]\n",
      " [55917.09 ]\n",
      " [55419.25 ]\n",
      " [54265.07 ]\n",
      " [55297.477]\n",
      " [55991.715]]\n"
     ]
    }
   ],
   "source": [
    "### Train on refit data and evaluate on test\n",
    "_, y_predict, _, _, test_rmse, kl_loss = train_model_and_evaluate_regression(Xrefit_tensor, Yrefit_tensor, Xtest_tensor, Ytest_tensor, layer, learning_param = lr, kl_weight = kl, steps = 100, printStep = False)\n",
    "print(\"Test RMSE: \",test_rmse)\n",
    "print(y_predict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**8. Obtain predictions for Backtesting**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X test tensor torch.Size([102, 9])\n",
      "Y test tensor torch.Size([102, 1])\n"
     ]
    }
   ],
   "source": [
    "Xfull_tensor, _, Yfull_tensor, _ = scale_and_convert_to_tensor(X_full, X_full, y_full, y_full, scaleTarget = True)\n",
    "print(\"X test tensor\",Xfull_tensor.shape)\n",
    "print(\"Y test tensor\",Yfull_tensor.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5645.038026420735"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model, y_predict = train_model_and_predict(Xfull_tensor, Yfull_tensor, layers = layer, learning_param = lr, kl_weight = kl, steps = 100)        \n",
    "math.sqrt(mean_squared_error(y_full,y_predict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pd.DataFrame(y_predict).to_csv(\"out.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## With Sentiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1. Create Feature Lags**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_lags = {\"Adj_Close_BTC-USD\" : 1, \n",
    "                \"Adj_Close_SPY\" : 1,\n",
    "                \"Adj_Close_GLD\" : 1,\n",
    "                \"Adj_Close_CHFUSD=X\" : 1,\n",
    "                \"Adj_Close_EURUSD=X\" : 1,\n",
    "                \"Adj_Close_GBPUSD=X\" : 1,\n",
    "                \"Adj_Close_JPYUSD=X\" : 1,\n",
    "                \"blockchain_transactions_per_block\" : 1,\n",
    "                \"blockchain_hash_rates\" : 1,\n",
    "                \"coindesk_sentiment\" : 1,\n",
    "                \"reddit_comments_sentiments\" : 1,\n",
    "                \"top_50_reddit_posts_sentiments\" : 1}\n",
    "\n",
    "data = lag(df, feature_lags)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2. Handle Train-test split**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data[(data[\"date\"] >= \"2021-01-01\") & (data[\"date\"] <= \"2021-04-12\")]\n",
    "\n",
    "## train, validation and test split\n",
    "train = data.loc[(data[\"date\"] >= \"2021-01-01\") & (data[\"date\"] <= \"2021-03-14\")]\n",
    "validation = data[(data[\"date\"] >= \"2021-03-15\") & (data[\"date\"] <= \"2021-03-29\")]\n",
    "test = data[(data[\"date\"] >= \"2021-03-30\") & (data[\"date\"] <= \"2021-04-12\")]\n",
    "\n",
    "## refit and full for later use \n",
    "refit = data[(data[\"date\"] >= \"2021-01-01\") & (data[\"date\"] <= \"2021-03-29\")]\n",
    "full = data.copy(deep = True)\n",
    "\n",
    "## train \n",
    "X_train = train.drop([\"date\", \"Adj_Close_BTC-USD\"], axis = 1)\n",
    "y_train = train[\"Adj_Close_BTC-USD\"]\n",
    "\n",
    "## validation\n",
    "X_val = validation.drop([\"date\", \"Adj_Close_BTC-USD\"], axis = 1)\n",
    "y_val = validation[\"Adj_Close_BTC-USD\"]\n",
    "\n",
    "## test\n",
    "X_test = test.drop([\"date\", \"Adj_Close_BTC-USD\"], axis = 1)\n",
    "y_test = test[\"Adj_Close_BTC-USD\"]\n",
    "\n",
    "## refit\n",
    "X_refit = refit.drop([\"date\", \"Adj_Close_BTC-USD\"], axis = 1)\n",
    "y_refit = refit[\"Adj_Close_BTC-USD\"]\n",
    "\n",
    "## full\n",
    "X_full = full.drop([\"date\", \"Adj_Close_BTC-USD\"], axis = 1)\n",
    "y_full = full[\"Adj_Close_BTC-USD\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3. Standardise dataset and transform to tensors** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X train tensor torch.Size([73, 12])\n",
      "Y train tensor torch.Size([73, 1])\n",
      "X val tensor torch.Size([15, 12])\n",
      "Y val tensor torch.Size([15, 1])\n",
      "X test tensor torch.Size([14, 12])\n",
      "Y test tensor torch.Size([14, 1])\n"
     ]
    }
   ],
   "source": [
    "## Standardise datasets and convert into tensors\n",
    "Xtrain_tensor, Xval_tensor, Ytrain_tensor, Yval_tensor = scale_and_convert_to_tensor(X_train, X_val, y_train, y_val, scaleTarget = True)\n",
    "_, Xtest_tensor, _, Ytest_tensor = scale_and_convert_to_tensor(X_train, X_test, y_train, y_test, scaleTarget = True)\n",
    "\n",
    "print(\"X train tensor\",Xtrain_tensor.shape)\n",
    "print(\"Y train tensor\",Ytrain_tensor.shape)\n",
    "\n",
    "print(\"X val tensor\",Xval_tensor.shape)\n",
    "print(\"Y val tensor\",Yval_tensor.shape)\n",
    "\n",
    "print(\"X test tensor\",Xtest_tensor.shape)\n",
    "print(\"Y test tensor\",Ytest_tensor.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4. Perform GridSearch**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Layer:  [32, 8]\n",
      "-- Learning Param:  0.001\n",
      "-- Learning Param:  0.011\n",
      "-- Learning Param:  0.021\n",
      "-- Learning Param:  0.032\n",
      "-- Learning Param:  0.042\n",
      "-- Learning Param:  0.052\n",
      "-- Learning Param:  0.062\n",
      "-- Learning Param:  0.072\n",
      "-- Learning Param:  0.082\n",
      "-- Learning Param:  0.093\n",
      "-- Learning Param:  0.103\n",
      "-- Learning Param:  0.113\n",
      "-- Learning Param:  0.123\n",
      "-- Learning Param:  0.133\n",
      "-- Learning Param:  0.144\n",
      "-- Learning Param:  0.154\n",
      "-- Learning Param:  0.164\n",
      "-- Learning Param:  0.174\n",
      "-- Learning Param:  0.184\n",
      "-- Learning Param:  0.194\n",
      "-- Learning Param:  0.205\n",
      "-- Learning Param:  0.215\n",
      "-- Learning Param:  0.225\n",
      "-- Learning Param:  0.235\n",
      "-- Learning Param:  0.245\n",
      "-- Learning Param:  0.256\n",
      "-- Learning Param:  0.266\n",
      "-- Learning Param:  0.276\n",
      "-- Learning Param:  0.286\n",
      "-- Learning Param:  0.296\n",
      "-- Learning Param:  0.307\n",
      "-- Learning Param:  0.317\n",
      "-- Learning Param:  0.327\n",
      "-- Learning Param:  0.337\n",
      "-- Learning Param:  0.347\n",
      "-- Learning Param:  0.357\n",
      "-- Learning Param:  0.368\n",
      "-- Learning Param:  0.378\n",
      "-- Learning Param:  0.388\n",
      "-- Learning Param:  0.398\n",
      "-- Learning Param:  0.408\n",
      "-- Learning Param:  0.419\n",
      "-- Learning Param:  0.429\n",
      "-- Learning Param:  0.439\n",
      "-- Learning Param:  0.449\n",
      "-- Learning Param:  0.459\n",
      "-- Learning Param:  0.469\n",
      "-- Learning Param:  0.48\n",
      "-- Learning Param:  0.49\n",
      "-- Learning Param:  0.5\n",
      "--- Layer:  [32, 16]\n",
      "-- Learning Param:  0.001\n",
      "-- Learning Param:  0.011\n",
      "-- Learning Param:  0.021\n",
      "-- Learning Param:  0.032\n",
      "-- Learning Param:  0.042\n",
      "-- Learning Param:  0.052\n",
      "-- Learning Param:  0.062\n",
      "-- Learning Param:  0.072\n",
      "-- Learning Param:  0.082\n",
      "-- Learning Param:  0.093\n",
      "-- Learning Param:  0.103\n",
      "-- Learning Param:  0.113\n",
      "-- Learning Param:  0.123\n",
      "-- Learning Param:  0.133\n",
      "-- Learning Param:  0.144\n",
      "-- Learning Param:  0.154\n",
      "-- Learning Param:  0.164\n",
      "-- Learning Param:  0.174\n",
      "-- Learning Param:  0.184\n",
      "-- Learning Param:  0.194\n",
      "-- Learning Param:  0.205\n",
      "-- Learning Param:  0.215\n",
      "-- Learning Param:  0.225\n",
      "-- Learning Param:  0.235\n",
      "-- Learning Param:  0.245\n",
      "-- Learning Param:  0.256\n",
      "-- Learning Param:  0.266\n",
      "-- Learning Param:  0.276\n",
      "-- Learning Param:  0.286\n",
      "-- Learning Param:  0.296\n",
      "-- Learning Param:  0.307\n",
      "-- Learning Param:  0.317\n",
      "-- Learning Param:  0.327\n",
      "-- Learning Param:  0.337\n",
      "-- Learning Param:  0.347\n",
      "-- Learning Param:  0.357\n",
      "-- Learning Param:  0.368\n",
      "-- Learning Param:  0.378\n",
      "-- Learning Param:  0.388\n",
      "-- Learning Param:  0.398\n",
      "-- Learning Param:  0.408\n",
      "-- Learning Param:  0.419\n",
      "-- Learning Param:  0.429\n",
      "-- Learning Param:  0.439\n",
      "-- Learning Param:  0.449\n",
      "-- Learning Param:  0.459\n",
      "-- Learning Param:  0.469\n",
      "-- Learning Param:  0.48\n",
      "-- Learning Param:  0.49\n",
      "-- Learning Param:  0.5\n",
      "--- Layer:  [32, 16, 8]\n",
      "-- Learning Param:  0.001\n",
      "-- Learning Param:  0.011\n",
      "-- Learning Param:  0.021\n",
      "-- Learning Param:  0.032\n",
      "-- Learning Param:  0.042\n",
      "-- Learning Param:  0.052\n",
      "-- Learning Param:  0.062\n",
      "-- Learning Param:  0.072\n",
      "-- Learning Param:  0.082\n",
      "-- Learning Param:  0.093\n",
      "-- Learning Param:  0.103\n",
      "-- Learning Param:  0.113\n",
      "-- Learning Param:  0.123\n",
      "-- Learning Param:  0.133\n",
      "-- Learning Param:  0.144\n",
      "-- Learning Param:  0.154\n",
      "-- Learning Param:  0.164\n",
      "-- Learning Param:  0.174\n",
      "-- Learning Param:  0.184\n",
      "-- Learning Param:  0.194\n",
      "-- Learning Param:  0.205\n",
      "-- Learning Param:  0.215\n",
      "-- Learning Param:  0.225\n",
      "-- Learning Param:  0.235\n",
      "-- Learning Param:  0.245\n",
      "-- Learning Param:  0.256\n",
      "-- Learning Param:  0.266\n",
      "-- Learning Param:  0.276\n",
      "-- Learning Param:  0.286\n",
      "-- Learning Param:  0.296\n",
      "-- Learning Param:  0.307\n",
      "-- Learning Param:  0.317\n",
      "-- Learning Param:  0.327\n",
      "-- Learning Param:  0.337\n",
      "-- Learning Param:  0.347\n",
      "-- Learning Param:  0.357\n",
      "-- Learning Param:  0.368\n",
      "-- Learning Param:  0.378\n",
      "-- Learning Param:  0.388\n",
      "-- Learning Param:  0.398\n",
      "-- Learning Param:  0.408\n",
      "-- Learning Param:  0.419\n",
      "-- Learning Param:  0.429\n",
      "-- Learning Param:  0.439\n",
      "-- Learning Param:  0.449\n",
      "-- Learning Param:  0.459\n",
      "-- Learning Param:  0.469\n",
      "-- Learning Param:  0.48\n",
      "-- Learning Param:  0.49\n",
      "-- Learning Param:  0.5\n",
      "Complete\n"
     ]
    }
   ],
   "source": [
    "training_mse_list = []\n",
    "val_mse_list = []\n",
    "val_rmse_list = []\n",
    "kl_list = []\n",
    "combination = []\n",
    "\n",
    "learning_param_list = pd.Series(np.linspace(0.001,0.5,50)).apply(lambda x: round(x,3))\n",
    "kl_weight =  pd.Series(np.linspace(0.001,0.5,50)).apply(lambda x: round(x,3))\n",
    "layers_list = ([32,8],[32,16],[32,16,8])\n",
    "\n",
    "for layer in layers_list:\n",
    "    print(\"--- Layer: \", layer)\n",
    "    for lr in learning_param_list:\n",
    "        print(\"-- Learning Param: \", lr)\n",
    "        for kl in kl_weight:\n",
    "            combination.append(\"layer: {} lr: {} kl: {}\".format(layer,lr,kl))\n",
    "            _ ,_, train_mse, val_mse, val_rmse, kl_loss = train_model_and_evaluate_regression(Xtrain_tensor, Ytrain_tensor, Xval_tensor, Yval_tensor, layer, learning_param = lr, kl_weight = kl, steps = 100, printStep = False)\n",
    "            training_mse_list.append(train_mse)\n",
    "            val_mse_list.append(val_mse)\n",
    "            val_rmse_list.append(val_rmse)\n",
    "            kl_list.append(kl_loss)\n",
    "            \n",
    "print(\"Complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4764    layer: [32, 16] lr: 0.459 kl: 0.144\n",
       "Name: Combination, dtype: object"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = pd.DataFrame({\"Combination\": combination,\"Train MSE\":training_mse_list, \"Val MSE\":val_mse_list, \"Val RMSE\":  val_rmse_list,\"KL Loss\":kl_list})\n",
    "results.to_csv(\"Combinations_regression_withSentiments&lagged2.csv\")\n",
    "\n",
    "## Find the hyperparameters with gives the lowest test RMSE\n",
    "results[results['Val RMSE'] ==  results['Val RMSE'].min()]['Combination']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val RMSE:  1432.557721350173\n"
     ]
    }
   ],
   "source": [
    "layer = [32, 16]\n",
    "lr =  0.459\n",
    "kl = 0.144\n",
    "\n",
    "model, y_predict, _, _, test_rmse, kl_loss = train_model_and_evaluate_regression(Xtrain_tensor, Ytrain_tensor, Xval_tensor, Yval_tensor, layer, learning_param = lr, kl_weight = kl, steps = 100, printStep = False)\n",
    "print(\"Val RMSE: \",test_rmse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**5. Retrain the model with selected hyperparameters and all data available.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X refit tensor torch.Size([88, 12])\n",
      "Y refit tensor torch.Size([88, 1])\n"
     ]
    }
   ],
   "source": [
    "### Standardise refit data (train + val)\n",
    "Xrefit_tensor, _, Yrefit_tensor, _ = scale_and_convert_to_tensor(X_refit, X_refit, y_refit, y_refit, scaleTarget = True)\n",
    "print(\"X refit tensor\",Xrefit_tensor.shape)\n",
    "print(\"Y refit tensor\",Yrefit_tensor.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test RMSE:  5358.489339356755\n",
      "[[55400.426]\n",
      " [56049.086]\n",
      " [57801.723]\n",
      " [57598.39 ]\n",
      " [57627.395]\n",
      " [61790.48 ]\n",
      " [58910.133]\n",
      " [64136.42 ]\n",
      " [66192.01 ]\n",
      " [61357.332]\n",
      " [63463.062]\n",
      " [65852.78 ]\n",
      " [67755.12 ]\n",
      " [69840.1  ]]\n"
     ]
    }
   ],
   "source": [
    "### Train on refit data and evaluate on test\n",
    "_, y_predict, _, _, test_rmse, kl_loss = train_model_and_evaluate_regression(Xrefit_tensor, Yrefit_tensor, Xtest_tensor, Ytest_tensor, layers = layer, learning_param = lr, kl_weight = kl, steps = 100, printStep = False)\n",
    "print(\"Test RMSE: \", test_rmse)\n",
    "print(y_predict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**6. Obtain predictions for backtest**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X test tensor torch.Size([102, 12])\n",
      "Y test tensor torch.Size([102, 1])\n"
     ]
    }
   ],
   "source": [
    "Xfull_tensor, _, Yfull_tensor, _ = scale_and_convert_to_tensor(X_full, X_full, y_full, y_full, scaleTarget = True)\n",
    "print(\"X test tensor\",Xfull_tensor.shape)\n",
    "print(\"Y test tensor\",Yfull_tensor.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9971.675252253297"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model, y_predict = train_model_and_predict(Xfull_tensor, Yfull_tensor, layers = layer, learning_param = lr, kl_weight = kl, steps = 100)        \n",
    "math.sqrt(mean_squared_error(y_full,y_predict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pd.DataFrame(y_predict).to_csv(\"out.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
