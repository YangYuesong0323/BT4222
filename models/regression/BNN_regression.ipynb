{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Bitcoin price prediction with Bayesian Neural Network Regression using torchBNN and PyTorch.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torchinfo import summary\n",
    "import torchbnn as bnn\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import r2_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "cwd = os.getcwd()\n",
    "from os.path import dirname, abspath\n",
    "while not cwd.endswith('BT4222_repo'):\n",
    "    cwd = os.path.dirname(cwd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>Adj_Close_BTC-USD</th>\n",
       "      <th>Open_BTC-USD</th>\n",
       "      <th>High_BTC-USD</th>\n",
       "      <th>Low_BTC-USD</th>\n",
       "      <th>Volume_BTC-USD</th>\n",
       "      <th>Adj_Close_SPY</th>\n",
       "      <th>Adj_Close_GLD</th>\n",
       "      <th>Adj_Close_CHFUSD=X</th>\n",
       "      <th>Adj_Close_CNYUSD=X</th>\n",
       "      <th>Adj_Close_EURUSD=X</th>\n",
       "      <th>Adj_Close_GBPUSD=X</th>\n",
       "      <th>Adj_Close_JPYUSD=X</th>\n",
       "      <th>coindesk_sentiment</th>\n",
       "      <th>num_of_coindesk_posts</th>\n",
       "      <th>reddit_comments_sentiments</th>\n",
       "      <th>top_50_reddit_posts_sentiments</th>\n",
       "      <th>blockchain_transactions_per_block</th>\n",
       "      <th>blockchain_hash_rates</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2020-12-14</td>\n",
       "      <td>19246.64453</td>\n",
       "      <td>19144.49219</td>\n",
       "      <td>19305.09961</td>\n",
       "      <td>19012.70898</td>\n",
       "      <td>22473997681</td>\n",
       "      <td>361.926788</td>\n",
       "      <td>171.539993</td>\n",
       "      <td>1.125442</td>\n",
       "      <td>0.152772</td>\n",
       "      <td>1.213340</td>\n",
       "      <td>1.331824</td>\n",
       "      <td>0.009621</td>\n",
       "      <td>0.249489</td>\n",
       "      <td>12</td>\n",
       "      <td>0.188275</td>\n",
       "      <td>0.297238</td>\n",
       "      <td>2167.931034</td>\n",
       "      <td>134533587.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2020-12-15</td>\n",
       "      <td>19417.07617</td>\n",
       "      <td>19246.91992</td>\n",
       "      <td>19525.00781</td>\n",
       "      <td>19079.84180</td>\n",
       "      <td>26741982541</td>\n",
       "      <td>366.819824</td>\n",
       "      <td>173.940002</td>\n",
       "      <td>1.127930</td>\n",
       "      <td>0.152679</td>\n",
       "      <td>1.214890</td>\n",
       "      <td>1.333084</td>\n",
       "      <td>0.009614</td>\n",
       "      <td>0.173773</td>\n",
       "      <td>18</td>\n",
       "      <td>0.144389</td>\n",
       "      <td>0.399427</td>\n",
       "      <td>2288.857143</td>\n",
       "      <td>133351912.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2020-12-16</td>\n",
       "      <td>21310.59766</td>\n",
       "      <td>19418.81836</td>\n",
       "      <td>21458.90820</td>\n",
       "      <td>19298.31641</td>\n",
       "      <td>44409011479</td>\n",
       "      <td>367.395508</td>\n",
       "      <td>174.899994</td>\n",
       "      <td>1.129382</td>\n",
       "      <td>0.152945</td>\n",
       "      <td>1.215430</td>\n",
       "      <td>1.344447</td>\n",
       "      <td>0.009649</td>\n",
       "      <td>0.341491</td>\n",
       "      <td>11</td>\n",
       "      <td>0.137256</td>\n",
       "      <td>0.489673</td>\n",
       "      <td>2204.314685</td>\n",
       "      <td>132323572.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2020-12-17</td>\n",
       "      <td>22805.16211</td>\n",
       "      <td>21308.35156</td>\n",
       "      <td>23642.66016</td>\n",
       "      <td>21234.67578</td>\n",
       "      <td>71378606374</td>\n",
       "      <td>369.449982</td>\n",
       "      <td>176.740005</td>\n",
       "      <td>1.129446</td>\n",
       "      <td>0.153109</td>\n",
       "      <td>1.219959</td>\n",
       "      <td>1.350293</td>\n",
       "      <td>0.009664</td>\n",
       "      <td>0.197572</td>\n",
       "      <td>10</td>\n",
       "      <td>0.156723</td>\n",
       "      <td>0.636030</td>\n",
       "      <td>2399.077519</td>\n",
       "      <td>132373208.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2020-12-18</td>\n",
       "      <td>23137.96094</td>\n",
       "      <td>22806.79688</td>\n",
       "      <td>23238.60156</td>\n",
       "      <td>22399.81250</td>\n",
       "      <td>40387896275</td>\n",
       "      <td>367.974792</td>\n",
       "      <td>176.440002</td>\n",
       "      <td>1.130301</td>\n",
       "      <td>0.153090</td>\n",
       "      <td>1.226272</td>\n",
       "      <td>1.357018</td>\n",
       "      <td>0.009696</td>\n",
       "      <td>0.315601</td>\n",
       "      <td>2</td>\n",
       "      <td>0.166419</td>\n",
       "      <td>0.107093</td>\n",
       "      <td>2392.031847</td>\n",
       "      <td>131791042.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        date  Adj_Close_BTC-USD  Open_BTC-USD  High_BTC-USD  Low_BTC-USD  \\\n",
       "0 2020-12-14        19246.64453   19144.49219   19305.09961  19012.70898   \n",
       "1 2020-12-15        19417.07617   19246.91992   19525.00781  19079.84180   \n",
       "2 2020-12-16        21310.59766   19418.81836   21458.90820  19298.31641   \n",
       "3 2020-12-17        22805.16211   21308.35156   23642.66016  21234.67578   \n",
       "4 2020-12-18        23137.96094   22806.79688   23238.60156  22399.81250   \n",
       "\n",
       "   Volume_BTC-USD  Adj_Close_SPY  Adj_Close_GLD  Adj_Close_CHFUSD=X  \\\n",
       "0     22473997681     361.926788     171.539993            1.125442   \n",
       "1     26741982541     366.819824     173.940002            1.127930   \n",
       "2     44409011479     367.395508     174.899994            1.129382   \n",
       "3     71378606374     369.449982     176.740005            1.129446   \n",
       "4     40387896275     367.974792     176.440002            1.130301   \n",
       "\n",
       "   Adj_Close_CNYUSD=X  Adj_Close_EURUSD=X  Adj_Close_GBPUSD=X  \\\n",
       "0            0.152772            1.213340            1.331824   \n",
       "1            0.152679            1.214890            1.333084   \n",
       "2            0.152945            1.215430            1.344447   \n",
       "3            0.153109            1.219959            1.350293   \n",
       "4            0.153090            1.226272            1.357018   \n",
       "\n",
       "   Adj_Close_JPYUSD=X  coindesk_sentiment  num_of_coindesk_posts  \\\n",
       "0            0.009621            0.249489                     12   \n",
       "1            0.009614            0.173773                     18   \n",
       "2            0.009649            0.341491                     11   \n",
       "3            0.009664            0.197572                     10   \n",
       "4            0.009696            0.315601                      2   \n",
       "\n",
       "   reddit_comments_sentiments  top_50_reddit_posts_sentiments  \\\n",
       "0                    0.188275                        0.297238   \n",
       "1                    0.144389                        0.399427   \n",
       "2                    0.137256                        0.489673   \n",
       "3                    0.156723                        0.636030   \n",
       "4                    0.166419                        0.107093   \n",
       "\n",
       "   blockchain_transactions_per_block  blockchain_hash_rates  \n",
       "0                        2167.931034            134533587.6  \n",
       "1                        2288.857143            133351912.2  \n",
       "2                        2204.314685            132323572.3  \n",
       "3                        2399.077519            132373208.7  \n",
       "4                        2392.031847            131791042.0  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PATH =\"../../data/cooked_data/cooked_complete_dataset.csv\"\n",
    "\n",
    "df = pd.read_csv(PATH,parse_dates =['date'])\n",
    "df.dropna(inplace = True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Helper Functions for creating lags and scaling**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lag(data, dic):\n",
    "    cols = []\n",
    "    for key, value in dic.items():\n",
    "        for i in range(1, value+1):\n",
    "            cols.append(data[key].shift(i).rename('{}_lag{}'.format(data[key].name, i)))\n",
    "    return pd.concat([data[\"date\"],data[\"Adj_Close_BTC-USD\"]] + cols, axis = 1)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "\n",
    "def scale_and_convert_to_tensor(Xtrain, Xtest, Ytrain, Ytest, scaleTarget = False):  \n",
    "    global scaler\n",
    "    \n",
    "    # Standardise features\n",
    "    Xtrain_standardised = scaler.fit_transform(Xtrain)\n",
    "    Xtest_standardised = scaler.transform(Xtest)\n",
    "    \n",
    "    # Standardise target\n",
    "    Ytrain_standardised = Ytrain\n",
    "    Ytest_standardised = Ytest\n",
    "    \n",
    "    if scaleTarget:    \n",
    "        Ytrain_standardised = scaler.fit_transform(np.array(Ytrain).reshape(-1, 1))\n",
    "        Ytest_standardised = scaler.transform(np.array(Ytest).reshape(-1, 1))\n",
    "    \n",
    "    ## Change to tensor\n",
    "    Xtrain_tensor = torch.from_numpy(Xtrain_standardised).float()\n",
    "    Ytrain_tensor = torch.from_numpy(np.array(Ytrain_standardised)).float()\n",
    "    Xtest_tensor = torch.from_numpy(Xtest_standardised).float()\n",
    "    Ytest_tensor = torch.from_numpy(np.array(Ytest_standardised)).float()\n",
    "        \n",
    "    return (Xtrain_tensor, Xtest_tensor, Ytrain_tensor, Ytest_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Without Sentiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1. Create feature lags**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_lags = {\"Adj_Close_BTC-USD\" : 2, \n",
    "                'Open_BTC-USD': 1, \n",
    "                'Low_BTC-USD': 1, \n",
    "                'High_BTC-USD': 1, \n",
    "                \"Volume_BTC-USD\" : 1, \n",
    "                \"Adj_Close_SPY\" : 1,\n",
    "                \"Adj_Close_GLD\" : 1,\n",
    "                \"Adj_Close_CHFUSD=X\" : 1,\n",
    "                \"Adj_Close_CNYUSD=X\" : 1,\n",
    "                \"Adj_Close_EURUSD=X\" : 1,\n",
    "                \"Adj_Close_GBPUSD=X\" : 1,\n",
    "                \"Adj_Close_JPYUSD=X\" : 1,\n",
    "                \"blockchain_transactions_per_block\" : 1,\n",
    "                \"blockchain_hash_rates\" : 1}\n",
    "\n",
    "data = lag(df, feature_lags)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2. Handle train-test split**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data[data[\"date\"] >= \"1/1/2021\"]\n",
    "\n",
    "train = data[data[\"date\"] <= \"2021-03-10\"]\n",
    "test = data[data[\"date\"] > \"2021-03-10\"]\n",
    "test = test[test[\"date\"] <= \"2021-04-5\"]\n",
    "\n",
    "X_train = train.drop([\"date\", \"Adj_Close_BTC-USD\"], axis = 1)\n",
    "y_train = train[\"Adj_Close_BTC-USD\"]\n",
    "\n",
    "X_test = test.drop([\"date\", \"Adj_Close_BTC-USD\"], axis = 1)\n",
    "y_test = test[\"Adj_Close_BTC-USD\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3. Standardise dataset and transform into tensors for pytorch**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X train tensor torch.Size([69, 15])\n",
      "Y train tensor torch.Size([69, 1])\n",
      "X test tensor torch.Size([26, 15])\n",
      "Y test tensor torch.Size([26, 1])\n"
     ]
    }
   ],
   "source": [
    "## Standardise datasets and convert into tensors\n",
    "Xtrain_tensor, Xtest_tensor, Ytrain_tensor, Ytest_tensor = scale_and_convert_to_tensor(X_train, X_test, y_train, y_test, scaleTarget = True)\n",
    "\n",
    "print(\"X train tensor\",Xtrain_tensor.shape)\n",
    "print(\"Y train tensor\",Ytrain_tensor.shape)\n",
    "print(\"X test tensor\",Xtest_tensor.shape)\n",
    "print(\"Y test tensor\",Ytest_tensor.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4. Define BNN training and evaluation pipeline**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model_and_evaluate_regression(Xtrain_tensor, Ytrain_tensor, Xtest_tensor, Ytest_tensor, layers = [32,8], learning_param = 0.01, kl_weight = 0.01, steps = 100, printStep = True):    \n",
    "    in_features = Xtrain_tensor.shape[1]\n",
    "    batch_size = Xtrain_tensor.shape[0]\n",
    "    \n",
    "    ## Ensure reproducibility\n",
    "    seed = 1\n",
    "    torch.manual_seed(seed)\n",
    "    \n",
    "    # Build model\n",
    "    layer = []\n",
    "    \n",
    "    ## Input layer\n",
    "    layer.append(bnn.BayesLinear(prior_mu=0, prior_sigma=0.1, in_features = in_features, out_features = layers[0]))\n",
    "    layer.append(nn.ReLU())\n",
    "    \n",
    "    ## Hidden layers\n",
    "    for index, neurons in enumerate(layers):\n",
    "        if index != (len(layers)-1):\n",
    "            layer.append(bnn.BayesLinear(prior_mu=0, prior_sigma=0.1, in_features=neurons, out_features=layers[index+1]))\n",
    "            layer.append(nn.ReLU())\n",
    "\n",
    "    ## Output layer\n",
    "    layer.append(bnn.BayesLinear(prior_mu=0, prior_sigma=0.1, in_features=layers[-1], out_features=1))\n",
    "    \n",
    "    model = nn.Sequential(*layer)\n",
    "    # Define Loss\n",
    "    mse_loss = nn.MSELoss()\n",
    "    kl_loss = bnn.BKLLoss(reduction='mean', last_layer_only=False)\n",
    "\n",
    "    ## Define optimiser with learning rate\n",
    "    optimizer = optim.Adam(model.parameters(), lr = learning_param)\n",
    "    \n",
    "    ### Train model\n",
    "    for step in range(steps):\n",
    "        torch.manual_seed(seed)\n",
    "        \n",
    "        pre = model(Xtrain_tensor)\n",
    "        mse = mse_loss(pre, Ytrain_tensor)\n",
    "        kl = kl_loss(model)\n",
    "        cost = mse + kl_weight*kl\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        cost.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Print Progress\n",
    "        if step%50==0 and printStep:\n",
    "            print('[Step %d]: MSE : %.8f, KL : %.8f' % (step , mse.item(), kl.item()))\n",
    "            \n",
    "    train_mse = mse.item()\n",
    "    kl_loss = kl.item()\n",
    "    \n",
    "    ## Predict Test\n",
    "    torch.manual_seed(seed)\n",
    "    y_predict = model(Xtest_tensor)\n",
    "    \n",
    "    ## Performance Evaluation on test - MSE\n",
    "    test_mse = mean_squared_error(Ytest_tensor.detach().numpy(),y_predict.detach().numpy())\n",
    "    \n",
    "    ## Inverse Standard Scaler - for unscaled RMSE \n",
    "    y_actual = scaler.inverse_transform(Ytest_tensor.detach().numpy().reshape(-1, 1))\n",
    "    y_predict = scaler.inverse_transform(y_predict.detach().numpy().reshape(-1, 1))\n",
    "    test_rmse = math.sqrt(mean_squared_error(y_actual,y_predict))\n",
    "    \n",
    "    return (model ,y_predict, train_mse, test_mse, test_rmse,kl_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**6. Perform Grid Search**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Layer:  [32, 16]\n",
      "-- Learning Param:  0.001\n",
      "-- Learning Param:  0.011\n",
      "-- Learning Param:  0.021\n",
      "-- Learning Param:  0.032\n",
      "-- Learning Param:  0.042\n",
      "-- Learning Param:  0.052\n",
      "-- Learning Param:  0.062\n",
      "-- Learning Param:  0.072\n",
      "-- Learning Param:  0.082\n",
      "-- Learning Param:  0.093\n",
      "-- Learning Param:  0.103\n",
      "-- Learning Param:  0.113\n",
      "-- Learning Param:  0.123\n",
      "-- Learning Param:  0.133\n",
      "-- Learning Param:  0.144\n",
      "-- Learning Param:  0.154\n",
      "-- Learning Param:  0.164\n",
      "-- Learning Param:  0.174\n",
      "-- Learning Param:  0.184\n",
      "-- Learning Param:  0.194\n",
      "-- Learning Param:  0.205\n",
      "-- Learning Param:  0.215\n",
      "-- Learning Param:  0.225\n",
      "-- Learning Param:  0.235\n",
      "-- Learning Param:  0.245\n",
      "-- Learning Param:  0.256\n",
      "-- Learning Param:  0.266\n",
      "-- Learning Param:  0.276\n",
      "-- Learning Param:  0.286\n",
      "-- Learning Param:  0.296\n",
      "-- Learning Param:  0.307\n",
      "-- Learning Param:  0.317\n",
      "-- Learning Param:  0.327\n",
      "-- Learning Param:  0.337\n",
      "-- Learning Param:  0.347\n",
      "-- Learning Param:  0.357\n",
      "-- Learning Param:  0.368\n",
      "-- Learning Param:  0.378\n",
      "-- Learning Param:  0.388\n",
      "-- Learning Param:  0.398\n",
      "-- Learning Param:  0.408\n",
      "-- Learning Param:  0.419\n",
      "-- Learning Param:  0.429\n",
      "-- Learning Param:  0.439\n",
      "-- Learning Param:  0.449\n",
      "-- Learning Param:  0.459\n",
      "-- Learning Param:  0.469\n",
      "-- Learning Param:  0.48\n",
      "-- Learning Param:  0.49\n",
      "-- Learning Param:  0.5\n",
      "--- Layer:  [32, 8]\n",
      "-- Learning Param:  0.001\n",
      "-- Learning Param:  0.011\n",
      "-- Learning Param:  0.021\n",
      "-- Learning Param:  0.032\n",
      "-- Learning Param:  0.042\n",
      "-- Learning Param:  0.052\n",
      "-- Learning Param:  0.062\n",
      "-- Learning Param:  0.072\n",
      "-- Learning Param:  0.082\n",
      "-- Learning Param:  0.093\n",
      "-- Learning Param:  0.103\n",
      "-- Learning Param:  0.113\n",
      "-- Learning Param:  0.123\n",
      "-- Learning Param:  0.133\n",
      "-- Learning Param:  0.144\n",
      "-- Learning Param:  0.154\n",
      "-- Learning Param:  0.164\n",
      "-- Learning Param:  0.174\n",
      "-- Learning Param:  0.184\n",
      "-- Learning Param:  0.194\n",
      "-- Learning Param:  0.205\n",
      "-- Learning Param:  0.215\n",
      "-- Learning Param:  0.225\n",
      "-- Learning Param:  0.235\n",
      "-- Learning Param:  0.245\n",
      "-- Learning Param:  0.256\n",
      "-- Learning Param:  0.266\n",
      "-- Learning Param:  0.276\n",
      "-- Learning Param:  0.286\n",
      "-- Learning Param:  0.296\n",
      "-- Learning Param:  0.307\n",
      "-- Learning Param:  0.317\n",
      "-- Learning Param:  0.327\n",
      "-- Learning Param:  0.337\n",
      "-- Learning Param:  0.347\n",
      "-- Learning Param:  0.357\n",
      "-- Learning Param:  0.368\n",
      "-- Learning Param:  0.378\n",
      "-- Learning Param:  0.388\n",
      "-- Learning Param:  0.398\n",
      "-- Learning Param:  0.408\n",
      "-- Learning Param:  0.419\n",
      "-- Learning Param:  0.429\n",
      "-- Learning Param:  0.439\n",
      "-- Learning Param:  0.449\n",
      "-- Learning Param:  0.459\n",
      "-- Learning Param:  0.469\n",
      "-- Learning Param:  0.48\n",
      "-- Learning Param:  0.49\n",
      "-- Learning Param:  0.5\n",
      "--- Layer:  [32, 16, 8]\n",
      "-- Learning Param:  0.001\n",
      "-- Learning Param:  0.011\n",
      "-- Learning Param:  0.021\n",
      "-- Learning Param:  0.032\n",
      "-- Learning Param:  0.042\n",
      "-- Learning Param:  0.052\n",
      "-- Learning Param:  0.062\n",
      "-- Learning Param:  0.072\n",
      "-- Learning Param:  0.082\n",
      "-- Learning Param:  0.093\n",
      "-- Learning Param:  0.103\n",
      "-- Learning Param:  0.113\n",
      "-- Learning Param:  0.123\n",
      "-- Learning Param:  0.133\n",
      "-- Learning Param:  0.144\n",
      "-- Learning Param:  0.154\n",
      "-- Learning Param:  0.164\n",
      "-- Learning Param:  0.174\n",
      "-- Learning Param:  0.184\n",
      "-- Learning Param:  0.194\n",
      "-- Learning Param:  0.205\n",
      "-- Learning Param:  0.215\n",
      "-- Learning Param:  0.225\n",
      "-- Learning Param:  0.235\n",
      "-- Learning Param:  0.245\n",
      "-- Learning Param:  0.256\n",
      "-- Learning Param:  0.266\n",
      "-- Learning Param:  0.276\n",
      "-- Learning Param:  0.286\n",
      "-- Learning Param:  0.296\n",
      "-- Learning Param:  0.307\n",
      "-- Learning Param:  0.317\n",
      "-- Learning Param:  0.327\n",
      "-- Learning Param:  0.337\n",
      "-- Learning Param:  0.347\n",
      "-- Learning Param:  0.357\n",
      "-- Learning Param:  0.368\n",
      "-- Learning Param:  0.378\n",
      "-- Learning Param:  0.388\n",
      "-- Learning Param:  0.398\n",
      "-- Learning Param:  0.408\n",
      "-- Learning Param:  0.419\n",
      "-- Learning Param:  0.429\n",
      "-- Learning Param:  0.439\n",
      "-- Learning Param:  0.449\n",
      "-- Learning Param:  0.459\n",
      "-- Learning Param:  0.469\n",
      "-- Learning Param:  0.48\n",
      "-- Learning Param:  0.49\n",
      "-- Learning Param:  0.5\n",
      "--- Layer:  [32, 20, 10]\n",
      "-- Learning Param:  0.001\n",
      "-- Learning Param:  0.011\n",
      "-- Learning Param:  0.021\n",
      "-- Learning Param:  0.032\n",
      "-- Learning Param:  0.042\n",
      "-- Learning Param:  0.052\n",
      "-- Learning Param:  0.062\n",
      "-- Learning Param:  0.072\n",
      "-- Learning Param:  0.082\n",
      "-- Learning Param:  0.093\n",
      "-- Learning Param:  0.103\n",
      "-- Learning Param:  0.113\n",
      "-- Learning Param:  0.123\n",
      "-- Learning Param:  0.133\n",
      "-- Learning Param:  0.144\n",
      "-- Learning Param:  0.154\n",
      "-- Learning Param:  0.164\n",
      "-- Learning Param:  0.174\n",
      "-- Learning Param:  0.184\n",
      "-- Learning Param:  0.194\n",
      "-- Learning Param:  0.205\n",
      "-- Learning Param:  0.215\n",
      "-- Learning Param:  0.225\n",
      "-- Learning Param:  0.235\n",
      "-- Learning Param:  0.245\n",
      "-- Learning Param:  0.256\n",
      "-- Learning Param:  0.266\n",
      "-- Learning Param:  0.276\n",
      "-- Learning Param:  0.286\n",
      "-- Learning Param:  0.296\n",
      "-- Learning Param:  0.307\n",
      "-- Learning Param:  0.317\n",
      "-- Learning Param:  0.327\n",
      "-- Learning Param:  0.337\n",
      "-- Learning Param:  0.347\n",
      "-- Learning Param:  0.357\n",
      "-- Learning Param:  0.368\n",
      "-- Learning Param:  0.378\n",
      "-- Learning Param:  0.388\n",
      "-- Learning Param:  0.398\n",
      "-- Learning Param:  0.408\n",
      "-- Learning Param:  0.419\n",
      "-- Learning Param:  0.429\n",
      "-- Learning Param:  0.439\n",
      "-- Learning Param:  0.449\n",
      "-- Learning Param:  0.459\n",
      "-- Learning Param:  0.469\n",
      "-- Learning Param:  0.48\n",
      "-- Learning Param:  0.49\n",
      "-- Learning Param:  0.5\n",
      "Complete\n"
     ]
    }
   ],
   "source": [
    "training_mse_list = []\n",
    "testing_mse_list = []\n",
    "testing_rmse_list = []\n",
    "kl_list = []\n",
    "combination = []\n",
    "\n",
    "learning_param_list = pd.Series(np.linspace(0.001,0.5,50)).apply(lambda x: round(x,3))\n",
    "kl_weight =  pd.Series(np.linspace(0.001,0.5,50)).apply(lambda x: round(x,3))\n",
    "layers_list = ([32,16],[32,8],[32,16,8],[32,20,10])\n",
    "\n",
    "for layer in layers_list:\n",
    "    print(\"--- Layer: \", layer)\n",
    "    for lr in learning_param_list:\n",
    "        print(\"-- Learning Param: \", lr)\n",
    "        for kl in kl_weight:\n",
    "            combination.append(\"layer: {} lr: {} kl: {}\".format(layer,lr,kl))\n",
    "            _ ,_, train_mse, test_mse, test_rmse,kl_loss = train_model_and_evaluate_regression(Xtrain_tensor, Ytrain_tensor, Xtest_tensor, Ytest_tensor, layer, learning_param = lr, kl_weight = kl, steps = 100, printStep = False)\n",
    "            training_mse_list.append(train_mse)\n",
    "            testing_mse_list.append(test_mse)\n",
    "            testing_rmse_list.append(test_rmse)\n",
    "            kl_list.append(kl_loss)\n",
    "            \n",
    "print(\"Complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5187    layer: [32, 16, 8] lr: 0.032 kl: 0.378\n",
       "Name: Combination, dtype: object"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = pd.DataFrame({\"Combination\": combination,\"Train MSE\":training_mse_list, \"Test MSE\":testing_mse_list, \"TEST RMSE\":  testing_rmse_list,\"KL Loss\":kl_list})\n",
    "results.to_csv(\"Combinations_regression_withoutSentiments&lagged2.csv\")\n",
    "\n",
    "## Find the hyperparameters with gives the lowest test RMSE\n",
    "results[results['TEST RMSE'] ==  results['TEST RMSE'].min()]['Combination']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test RMSE:  1610.9847919828417\n"
     ]
    }
   ],
   "source": [
    "layer = [32, 16, 8]\n",
    "lr = 0.032\n",
    "kl = 0.378\n",
    "\n",
    "model, y_predict, _, _, test_rmse, kl_loss = train_model_and_evaluate_regression(Xtrain_tensor, Ytrain_tensor, Xtest_tensor, Ytest_tensor, layer, learning_param = lr, kl_weight = kl, steps = 100, printStep = False)\n",
    "print(\"Test RMSE: \",test_rmse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**7. Retrain the model with selected hyperparameters and all train data available**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Helper function for retraining the model with all data available\n",
    "\n",
    "def train_model_and_predict(Xtrain_tensor, Ytrain_tensor, layers = [32,8], learning_param = 0.01, kl_weight = 0.01, steps = 100):    \n",
    "    \"\"\" \n",
    "    Trains model and returns predictions on entire dataset.\n",
    "    \"\"\"\n",
    "    in_features = Xtrain_tensor.shape[1]\n",
    "    batch_size = Xtrain_tensor.shape[0]\n",
    "    \n",
    "    ## Ensure reproducibility\n",
    "    seed = 1\n",
    "    torch.manual_seed(seed)\n",
    "    \n",
    "    # Build model\n",
    "    layer = []\n",
    "    \n",
    "    ## Input layer\n",
    "    layer.append(bnn.BayesLinear(prior_mu=0, prior_sigma=0.1, in_features = in_features, out_features = layers[0]))\n",
    "    layer.append(nn.ReLU())\n",
    "    \n",
    "    ## Hidden layers\n",
    "    for index, neurons in enumerate(layers):\n",
    "        if index != (len(layers)-1):\n",
    "            layer.append(bnn.BayesLinear(prior_mu=0, prior_sigma=0.1, in_features=neurons, out_features=layers[index+1]))\n",
    "            layer.append(nn.ReLU())\n",
    "\n",
    "    ## Output layer\n",
    "    layer.append(bnn.BayesLinear(prior_mu=0, prior_sigma=0.1, in_features=layers[-1], out_features=1))\n",
    "    \n",
    "    model = nn.Sequential(*layer)\n",
    "    # Define Loss\n",
    "    mse_loss = nn.MSELoss()\n",
    "    kl_loss = bnn.BKLLoss(reduction='mean', last_layer_only=False)\n",
    "\n",
    "    ## Define optimiser with learning rate\n",
    "    optimizer = optim.Adam(model.parameters(), lr = learning_param)\n",
    "    \n",
    "    ### Train model\n",
    "    for step in range(steps):\n",
    "        torch.manual_seed(seed)\n",
    "        \n",
    "        pre = model(Xtrain_tensor)\n",
    "        mse = mse_loss(pre, Ytrain_tensor)\n",
    "        kl = kl_loss(model)\n",
    "        cost = mse + kl_weight*kl\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        cost.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "    ## Predict Test\n",
    "    torch.manual_seed(seed)\n",
    "    y_predict = model(Xtrain_tensor)\n",
    "\n",
    "    ## Inverse Standard Scaler\n",
    "    y_predict = scaler.inverse_transform(y_predict.detach().numpy().reshape(-1, 1))\n",
    "    \n",
    "    return (model,y_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Obtain all available data\n",
    "x_refit = data.drop([\"date\", \"Adj_Close_BTC-USD\"], axis = 1)\n",
    "y_refit = data[\"Adj_Close_BTC-USD\"]\n",
    "\n",
    "### Used helper standardise function to create test and train but Xtrain_tensor is equal to Xtest_tensor\n",
    "Xtrain_tensor, Xtest_tensor, Ytrain_tensor, Ytest_tensor = scale_and_convert_to_tensor(x_refit, x_refit, y_refit, y_refit, scaleTarget = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, y_predict = train_model_and_predict(Xtrain_tensor, Ytrain_tensor, layers = [32, 16, 8], learning_param = 0.032, kl_weight = 0.266, steps = 100)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pd.DataFrame(y_predict).to_csv(\"out.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## With Sentiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1. Create Feature Lags**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_lags = {\"Adj_Close_BTC-USD\" : 2, \n",
    "                'Open_BTC-USD': 1, \n",
    "                'Low_BTC-USD': 1, \n",
    "                'High_BTC-USD': 1, \n",
    "                \"Volume_BTC-USD\" : 1, \n",
    "                \"Adj_Close_SPY\" : 1,\n",
    "                \"Adj_Close_GLD\" : 1,\n",
    "                \"Adj_Close_CHFUSD=X\" : 1,\n",
    "                \"Adj_Close_CNYUSD=X\" : 1,\n",
    "                \"Adj_Close_EURUSD=X\" : 1,\n",
    "                \"Adj_Close_GBPUSD=X\" : 1,\n",
    "                \"Adj_Close_JPYUSD=X\" : 1,\n",
    "                \"blockchain_transactions_per_block\" : 1,\n",
    "                \"blockchain_hash_rates\" : 1,\n",
    "                \"coindesk_sentiment\" : 1,\n",
    "                \"num_of_coindesk_posts\" : 1,\n",
    "                \"reddit_comments_sentiments\" : 1,\n",
    "                \"top_50_reddit_posts_sentiments\" : 1}\n",
    "\n",
    "data = lag(df, feature_lags)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2. Handle Train-test split**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data[data[\"date\"] >= \"1/1/2021\"]\n",
    "\n",
    "train = data[data[\"date\"] <= \"2021-03-10\"]\n",
    "test = data[data[\"date\"] > \"2021-03-10\"]\n",
    "test = test[test[\"date\"] <= \"2021-04-5\"]\n",
    "\n",
    "X_train = train.drop([\"date\", \"Adj_Close_BTC-USD\"], axis = 1)\n",
    "y_train = train[\"Adj_Close_BTC-USD\"]\n",
    "\n",
    "X_test = test.drop([\"date\", \"Adj_Close_BTC-USD\"], axis = 1)\n",
    "y_test = test[\"Adj_Close_BTC-USD\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3. Standardise dataset and transform to tensors** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X train tensor torch.Size([69, 19])\n",
      "Y train tensor torch.Size([69, 1])\n",
      "X test tensor torch.Size([26, 19])\n",
      "Y test tensor torch.Size([26, 1])\n"
     ]
    }
   ],
   "source": [
    "## Standardise datasets and convert into tensors\n",
    "Xtrain_tensor, Xtest_tensor, Ytrain_tensor, Ytest_tensor = scale_and_convert_to_tensor(X_train, X_test, y_train, y_test, scaleTarget = True)\n",
    "\n",
    "print(\"X train tensor\",Xtrain_tensor.shape)\n",
    "print(\"Y train tensor\",Ytrain_tensor.shape)\n",
    "print(\"X test tensor\",Xtest_tensor.shape)\n",
    "print(\"Y test tensor\",Ytest_tensor.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4. Perform GridSearch**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Layer:  [32, 16]\n",
      "-- Learning Param:  0.001\n",
      "-- Learning Param:  0.011\n",
      "-- Learning Param:  0.021\n",
      "-- Learning Param:  0.032\n",
      "-- Learning Param:  0.042\n",
      "-- Learning Param:  0.052\n",
      "-- Learning Param:  0.062\n",
      "-- Learning Param:  0.072\n",
      "-- Learning Param:  0.082\n",
      "-- Learning Param:  0.093\n",
      "-- Learning Param:  0.103\n",
      "-- Learning Param:  0.113\n",
      "-- Learning Param:  0.123\n",
      "-- Learning Param:  0.133\n",
      "-- Learning Param:  0.144\n",
      "-- Learning Param:  0.154\n",
      "-- Learning Param:  0.164\n",
      "-- Learning Param:  0.174\n",
      "-- Learning Param:  0.184\n",
      "-- Learning Param:  0.194\n",
      "-- Learning Param:  0.205\n",
      "-- Learning Param:  0.215\n",
      "-- Learning Param:  0.225\n",
      "-- Learning Param:  0.235\n",
      "-- Learning Param:  0.245\n",
      "-- Learning Param:  0.256\n",
      "-- Learning Param:  0.266\n",
      "-- Learning Param:  0.276\n",
      "-- Learning Param:  0.286\n",
      "-- Learning Param:  0.296\n",
      "-- Learning Param:  0.307\n",
      "-- Learning Param:  0.317\n",
      "-- Learning Param:  0.327\n",
      "-- Learning Param:  0.337\n",
      "-- Learning Param:  0.347\n",
      "-- Learning Param:  0.357\n",
      "-- Learning Param:  0.368\n",
      "-- Learning Param:  0.378\n",
      "-- Learning Param:  0.388\n",
      "-- Learning Param:  0.398\n",
      "-- Learning Param:  0.408\n",
      "-- Learning Param:  0.419\n",
      "-- Learning Param:  0.429\n",
      "-- Learning Param:  0.439\n",
      "-- Learning Param:  0.449\n",
      "-- Learning Param:  0.459\n",
      "-- Learning Param:  0.469\n",
      "-- Learning Param:  0.48\n",
      "-- Learning Param:  0.49\n",
      "-- Learning Param:  0.5\n",
      "--- Layer:  [32, 8]\n",
      "-- Learning Param:  0.001\n",
      "-- Learning Param:  0.011\n",
      "-- Learning Param:  0.021\n",
      "-- Learning Param:  0.032\n",
      "-- Learning Param:  0.042\n",
      "-- Learning Param:  0.052\n",
      "-- Learning Param:  0.062\n",
      "-- Learning Param:  0.072\n",
      "-- Learning Param:  0.082\n",
      "-- Learning Param:  0.093\n",
      "-- Learning Param:  0.103\n",
      "-- Learning Param:  0.113\n",
      "-- Learning Param:  0.123\n",
      "-- Learning Param:  0.133\n",
      "-- Learning Param:  0.144\n",
      "-- Learning Param:  0.154\n",
      "-- Learning Param:  0.164\n",
      "-- Learning Param:  0.174\n",
      "-- Learning Param:  0.184\n",
      "-- Learning Param:  0.194\n",
      "-- Learning Param:  0.205\n",
      "-- Learning Param:  0.215\n",
      "-- Learning Param:  0.225\n",
      "-- Learning Param:  0.235\n",
      "-- Learning Param:  0.245\n",
      "-- Learning Param:  0.256\n",
      "-- Learning Param:  0.266\n",
      "-- Learning Param:  0.276\n",
      "-- Learning Param:  0.286\n",
      "-- Learning Param:  0.296\n",
      "-- Learning Param:  0.307\n",
      "-- Learning Param:  0.317\n",
      "-- Learning Param:  0.327\n",
      "-- Learning Param:  0.337\n",
      "-- Learning Param:  0.347\n",
      "-- Learning Param:  0.357\n",
      "-- Learning Param:  0.368\n",
      "-- Learning Param:  0.378\n",
      "-- Learning Param:  0.388\n",
      "-- Learning Param:  0.398\n",
      "-- Learning Param:  0.408\n",
      "-- Learning Param:  0.419\n",
      "-- Learning Param:  0.429\n",
      "-- Learning Param:  0.439\n",
      "-- Learning Param:  0.449\n",
      "-- Learning Param:  0.459\n",
      "-- Learning Param:  0.469\n",
      "-- Learning Param:  0.48\n",
      "-- Learning Param:  0.49\n",
      "-- Learning Param:  0.5\n",
      "--- Layer:  [32, 16, 8]\n",
      "-- Learning Param:  0.001\n",
      "-- Learning Param:  0.011\n",
      "-- Learning Param:  0.021\n",
      "-- Learning Param:  0.032\n",
      "-- Learning Param:  0.042\n",
      "-- Learning Param:  0.052\n",
      "-- Learning Param:  0.062\n",
      "-- Learning Param:  0.072\n",
      "-- Learning Param:  0.082\n",
      "-- Learning Param:  0.093\n",
      "-- Learning Param:  0.103\n",
      "-- Learning Param:  0.113\n",
      "-- Learning Param:  0.123\n",
      "-- Learning Param:  0.133\n",
      "-- Learning Param:  0.144\n",
      "-- Learning Param:  0.154\n",
      "-- Learning Param:  0.164\n",
      "-- Learning Param:  0.174\n",
      "-- Learning Param:  0.184\n",
      "-- Learning Param:  0.194\n",
      "-- Learning Param:  0.205\n",
      "-- Learning Param:  0.215\n",
      "-- Learning Param:  0.225\n",
      "-- Learning Param:  0.235\n",
      "-- Learning Param:  0.245\n",
      "-- Learning Param:  0.256\n",
      "-- Learning Param:  0.266\n",
      "-- Learning Param:  0.276\n",
      "-- Learning Param:  0.286\n",
      "-- Learning Param:  0.296\n",
      "-- Learning Param:  0.307\n",
      "-- Learning Param:  0.317\n",
      "-- Learning Param:  0.327\n",
      "-- Learning Param:  0.337\n",
      "-- Learning Param:  0.347\n",
      "-- Learning Param:  0.357\n",
      "-- Learning Param:  0.368\n",
      "-- Learning Param:  0.378\n",
      "-- Learning Param:  0.388\n",
      "-- Learning Param:  0.398\n",
      "-- Learning Param:  0.408\n",
      "-- Learning Param:  0.419\n",
      "-- Learning Param:  0.429\n",
      "-- Learning Param:  0.439\n",
      "-- Learning Param:  0.449\n",
      "-- Learning Param:  0.459\n",
      "-- Learning Param:  0.469\n",
      "-- Learning Param:  0.48\n",
      "-- Learning Param:  0.49\n",
      "-- Learning Param:  0.5\n",
      "Complete\n"
     ]
    }
   ],
   "source": [
    "training_mse_list = []\n",
    "testing_mse_list = []\n",
    "testing_rmse_list = []\n",
    "kl_list = []\n",
    "combination = []\n",
    "\n",
    "learning_param_list = pd.Series(np.linspace(0.001,0.5,50)).apply(lambda x: round(x,3))\n",
    "kl_weight =  pd.Series(np.linspace(0.001,0.5,50)).apply(lambda x: round(x,3))\n",
    "layers_list = ([32,16],[32,8],[32,16,8])\n",
    "\n",
    "for layer in layers_list:\n",
    "    print(\"--- Layer: \", layer)\n",
    "    for lr in learning_param_list:\n",
    "        print(\"-- Learning Param: \", lr)\n",
    "        for kl in kl_weight:\n",
    "            combination.append(\"layer: {} lr: {} kl: {}\".format(layer,lr,kl))\n",
    "            _ ,_, train_mse, test_mse, test_rmse,kl_loss = train_model_and_evaluate_regression(Xtrain_tensor, Ytrain_tensor, Xtest_tensor, Ytest_tensor, layer, learning_param = lr, kl_weight = kl, steps = 100, printStep = False)\n",
    "            training_mse_list.append(train_mse)\n",
    "            testing_mse_list.append(test_mse)\n",
    "            testing_rmse_list.append(test_rmse)\n",
    "            kl_list.append(kl_loss)\n",
    "            \n",
    "print(\"Complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3037    layer: [32, 8] lr: 0.103 kl: 0.378\n",
       "Name: Combination, dtype: object"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_withSentiments = pd.DataFrame({\"Combination\": combination,\"Train MSE\":training_mse_list, \"Test MSE\":testing_mse_list, \"TEST RMSE\":  testing_rmse_list,\"KL Loss\":kl_list})\n",
    "results_withSentiments.to_csv(\"Combinations_regression_withSentiments&lagged2.csv\")\n",
    "\n",
    "## Find the parameters with gives the lowest test RMSE\n",
    "results_withSentiments[results_withSentiments['TEST RMSE'] ==  results_withSentiments['TEST RMSE'].min()]['Combination']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test RMSE:  1866.2615036484035\n"
     ]
    }
   ],
   "source": [
    "layer = [32, 8]\n",
    "lr = 0.103\n",
    "kl = 0.378\n",
    "\n",
    "model, y_predict, _, _, test_rmse, kl_loss = train_model_and_evaluate_regression(Xtrain_tensor, Ytrain_tensor, Xtest_tensor, Ytest_tensor, layer, learning_param = lr, kl_weight = kl, steps = 100, printStep = False)\n",
    "print(\"Test RMSE: \", test_rmse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**5. Retrain the model with selected hyperparameters and all data available.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Obtain all available data\n",
    "x_refit = data.drop([\"date\", \"Adj_Close_BTC-USD\"], axis = 1)\n",
    "y_refit = data[\"Adj_Close_BTC-USD\"]\n",
    "\n",
    "### Used helper standardise function to create test and train but Xtrain_tensor is equal to Xtest_tensor\n",
    "Xtrain_tensor, Xtest_tensor, Ytrain_tensor, Ytest_tensor = scale_and_convert_to_tensor(x_refit, x_refit, y_refit, y_refit, scaleTarget = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, y_predict = train_model_and_predict(Xtrain_tensor, Ytrain_tensor, layers = [32, 16, 8], learning_param = 0.032, kl_weight = 0.266, steps = 100)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pd.DataFrame(y_predict).to_csv(\"out.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
