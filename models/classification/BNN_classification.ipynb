{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Bitcoin price prediction with Bayesian Neural Network Classification using torchBNN and PyTorch.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, roc_auc_score\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torchinfo import summary\n",
    "import torchbnn as bnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "PATH =\"../../data/cooked_data/cooked_complete_dataset.csvv\"\n",
    "df = pd.read_csv(PATH,parse_dates =['date'])\n",
    "df.dropna(inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>Adj_Close_BTC-USD</th>\n",
       "      <th>Open_BTC-USD</th>\n",
       "      <th>High_BTC-USD</th>\n",
       "      <th>Low_BTC-USD</th>\n",
       "      <th>Volume_BTC-USD</th>\n",
       "      <th>Adj_Close_SPY</th>\n",
       "      <th>Adj_Close_GLD</th>\n",
       "      <th>Adj_Close_CHFUSD=X</th>\n",
       "      <th>Adj_Close_CNYUSD=X</th>\n",
       "      <th>Adj_Close_EURUSD=X</th>\n",
       "      <th>Adj_Close_GBPUSD=X</th>\n",
       "      <th>Adj_Close_JPYUSD=X</th>\n",
       "      <th>coindesk_sentiment</th>\n",
       "      <th>num_of_coindesk_posts</th>\n",
       "      <th>reddit_comments_sentiments</th>\n",
       "      <th>top_50_reddit_posts_sentiments</th>\n",
       "      <th>blockchain_transactions_per_block</th>\n",
       "      <th>blockchain_hash_rates</th>\n",
       "      <th>class_y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2020-12-14</td>\n",
       "      <td>19246.64453</td>\n",
       "      <td>19144.49219</td>\n",
       "      <td>19305.09961</td>\n",
       "      <td>19012.70898</td>\n",
       "      <td>22473997681</td>\n",
       "      <td>361.926788</td>\n",
       "      <td>171.539993</td>\n",
       "      <td>1.125442</td>\n",
       "      <td>0.152772</td>\n",
       "      <td>1.213340</td>\n",
       "      <td>1.331824</td>\n",
       "      <td>0.009621</td>\n",
       "      <td>0.249489</td>\n",
       "      <td>12</td>\n",
       "      <td>0.188275</td>\n",
       "      <td>0.297238</td>\n",
       "      <td>2167.931034</td>\n",
       "      <td>134533587.6</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2020-12-15</td>\n",
       "      <td>19417.07617</td>\n",
       "      <td>19246.91992</td>\n",
       "      <td>19525.00781</td>\n",
       "      <td>19079.84180</td>\n",
       "      <td>26741982541</td>\n",
       "      <td>366.819824</td>\n",
       "      <td>173.940002</td>\n",
       "      <td>1.127930</td>\n",
       "      <td>0.152679</td>\n",
       "      <td>1.214890</td>\n",
       "      <td>1.333084</td>\n",
       "      <td>0.009614</td>\n",
       "      <td>0.173773</td>\n",
       "      <td>18</td>\n",
       "      <td>0.144389</td>\n",
       "      <td>0.399427</td>\n",
       "      <td>2288.857143</td>\n",
       "      <td>133351912.2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2020-12-16</td>\n",
       "      <td>21310.59766</td>\n",
       "      <td>19418.81836</td>\n",
       "      <td>21458.90820</td>\n",
       "      <td>19298.31641</td>\n",
       "      <td>44409011479</td>\n",
       "      <td>367.395508</td>\n",
       "      <td>174.899994</td>\n",
       "      <td>1.129382</td>\n",
       "      <td>0.152945</td>\n",
       "      <td>1.215430</td>\n",
       "      <td>1.344447</td>\n",
       "      <td>0.009649</td>\n",
       "      <td>0.341491</td>\n",
       "      <td>11</td>\n",
       "      <td>0.137256</td>\n",
       "      <td>0.489673</td>\n",
       "      <td>2204.314685</td>\n",
       "      <td>132323572.3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2020-12-17</td>\n",
       "      <td>22805.16211</td>\n",
       "      <td>21308.35156</td>\n",
       "      <td>23642.66016</td>\n",
       "      <td>21234.67578</td>\n",
       "      <td>71378606374</td>\n",
       "      <td>369.449982</td>\n",
       "      <td>176.740005</td>\n",
       "      <td>1.129446</td>\n",
       "      <td>0.153109</td>\n",
       "      <td>1.219959</td>\n",
       "      <td>1.350293</td>\n",
       "      <td>0.009664</td>\n",
       "      <td>0.197572</td>\n",
       "      <td>10</td>\n",
       "      <td>0.156723</td>\n",
       "      <td>0.636030</td>\n",
       "      <td>2399.077519</td>\n",
       "      <td>132373208.7</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2020-12-18</td>\n",
       "      <td>23137.96094</td>\n",
       "      <td>22806.79688</td>\n",
       "      <td>23238.60156</td>\n",
       "      <td>22399.81250</td>\n",
       "      <td>40387896275</td>\n",
       "      <td>367.974792</td>\n",
       "      <td>176.440002</td>\n",
       "      <td>1.130301</td>\n",
       "      <td>0.153090</td>\n",
       "      <td>1.226272</td>\n",
       "      <td>1.357018</td>\n",
       "      <td>0.009696</td>\n",
       "      <td>0.315601</td>\n",
       "      <td>2</td>\n",
       "      <td>0.166419</td>\n",
       "      <td>0.107093</td>\n",
       "      <td>2392.031847</td>\n",
       "      <td>131791042.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        date  Adj_Close_BTC-USD  Open_BTC-USD  High_BTC-USD  Low_BTC-USD  \\\n",
       "0 2020-12-14        19246.64453   19144.49219   19305.09961  19012.70898   \n",
       "1 2020-12-15        19417.07617   19246.91992   19525.00781  19079.84180   \n",
       "2 2020-12-16        21310.59766   19418.81836   21458.90820  19298.31641   \n",
       "3 2020-12-17        22805.16211   21308.35156   23642.66016  21234.67578   \n",
       "4 2020-12-18        23137.96094   22806.79688   23238.60156  22399.81250   \n",
       "\n",
       "   Volume_BTC-USD  Adj_Close_SPY  Adj_Close_GLD  Adj_Close_CHFUSD=X  \\\n",
       "0     22473997681     361.926788     171.539993            1.125442   \n",
       "1     26741982541     366.819824     173.940002            1.127930   \n",
       "2     44409011479     367.395508     174.899994            1.129382   \n",
       "3     71378606374     369.449982     176.740005            1.129446   \n",
       "4     40387896275     367.974792     176.440002            1.130301   \n",
       "\n",
       "   Adj_Close_CNYUSD=X  Adj_Close_EURUSD=X  Adj_Close_GBPUSD=X  \\\n",
       "0            0.152772            1.213340            1.331824   \n",
       "1            0.152679            1.214890            1.333084   \n",
       "2            0.152945            1.215430            1.344447   \n",
       "3            0.153109            1.219959            1.350293   \n",
       "4            0.153090            1.226272            1.357018   \n",
       "\n",
       "   Adj_Close_JPYUSD=X  coindesk_sentiment  num_of_coindesk_posts  \\\n",
       "0            0.009621            0.249489                     12   \n",
       "1            0.009614            0.173773                     18   \n",
       "2            0.009649            0.341491                     11   \n",
       "3            0.009664            0.197572                     10   \n",
       "4            0.009696            0.315601                      2   \n",
       "\n",
       "   reddit_comments_sentiments  top_50_reddit_posts_sentiments  \\\n",
       "0                    0.188275                        0.297238   \n",
       "1                    0.144389                        0.399427   \n",
       "2                    0.137256                        0.489673   \n",
       "3                    0.156723                        0.636030   \n",
       "4                    0.166419                        0.107093   \n",
       "\n",
       "   blockchain_transactions_per_block  blockchain_hash_rates  class_y  \n",
       "0                        2167.931034            134533587.6        0  \n",
       "1                        2288.857143            133351912.2        1  \n",
       "2                        2204.314685            132323572.3        1  \n",
       "3                        2399.077519            132373208.7        1  \n",
       "4                        2392.031847            131791042.0        1  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Create binary label\n",
    "df[\"class_y\"] = df[\"Adj_Close_BTC-USD\"].shift(1).dropna()\n",
    "df[\"class_y\"] = df.apply(lambda x : 1 if x[\"class_y\"] < x[\"Adj_Close_BTC-USD\"] else 0 , axis = 1)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Helper Functions for creating lags and scaling.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lag(data, dic):\n",
    "    cols = []\n",
    "    for key, value in dic.items():\n",
    "        for i in range(1, value+1):\n",
    "            cols.append(data[key].shift(i).rename('{}_lag{}'.format(data[key].name, i)))\n",
    "    return pd.concat([data[\"date\"],data[\"class_y\"]] + cols, axis = 1)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "\n",
    "def scale_and_convert_to_tensor(Xtrain, Xtest, Ytrain, Ytest, scaleTarget = False):  \n",
    "    global scaler\n",
    "    \n",
    "    # Standardise features\n",
    "    Xtrain_standardised = scaler.fit_transform(Xtrain)\n",
    "    Xtest_standardised = scaler.transform(Xtest)\n",
    "    \n",
    "    # Standardise target\n",
    "    Ytrain_standardised = Ytrain\n",
    "    Ytest_standardised = Ytest\n",
    "    \n",
    "    if scaleTarget:    \n",
    "        Ytrain_standardised = scaler.fit_transform(np.array(Ytrain).reshape(-1, 1))\n",
    "        Ytest_standardised = scaler.transform(np.array(Ytest).reshape(-1, 1))\n",
    "    \n",
    "    ## Change to tensor\n",
    "    Xtrain_tensor = torch.from_numpy(Xtrain_standardised).float()\n",
    "    Ytrain_tensor = torch.from_numpy(np.array(Ytrain_standardised)).float()\n",
    "    Xtest_tensor = torch.from_numpy(Xtest_standardised).float()\n",
    "    Ytest_tensor = torch.from_numpy(np.array(Ytest_standardised)).float()\n",
    "        \n",
    "    return (Xtrain_tensor, Xtest_tensor, Ytrain_tensor, Ytest_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Without Sentiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1. Create feature lags**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_lags = {\"Adj_Close_BTC-USD\" : 2, \n",
    "                'Open_BTC-USD': 1, \n",
    "                'Low_BTC-USD': 1, \n",
    "                'High_BTC-USD': 1,\n",
    "                \"Volume_BTC-USD\" : 1, \n",
    "                \"Adj_Close_SPY\" : 1,\n",
    "                \"Adj_Close_GLD\" : 1,\n",
    "                \"Adj_Close_CHFUSD=X\" : 1,\n",
    "                \"Adj_Close_CNYUSD=X\" : 1,\n",
    "                \"Adj_Close_EURUSD=X\" : 1,\n",
    "                \"Adj_Close_GBPUSD=X\" : 1,\n",
    "                \"Adj_Close_JPYUSD=X\" : 1,\n",
    "                \"blockchain_transactions_per_block\" : 1,\n",
    "                \"blockchain_hash_rates\" : 1}\n",
    "\n",
    "data = lag(df, feature_lags)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2. Handle train-test split**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data[data[\"date\"] >= \"1/1/2021\"]\n",
    "\n",
    "train = data[data[\"date\"] <= \"2021-03-10\"]\n",
    "test = data[(data[\"date\"] > \"2021-03-10\") & (data[\"date\"] <= \"2021-04-5\")]\n",
    "\n",
    "X_train = train.drop([\"date\", \"class_y\"], axis = 1)\n",
    "y_train = train[\"class_y\"]\n",
    "\n",
    "X_test = test.drop([\"date\", \"class_y\"], axis = 1)\n",
    "y_test = test[\"class_y\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3. Standardise dataset and transform into tensors for pytorch**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X train tensor torch.Size([69, 15])\n",
      "Y train tensor torch.Size([69])\n",
      "X test tensor torch.Size([26, 15])\n",
      "Y test tensor torch.Size([26])\n"
     ]
    }
   ],
   "source": [
    "Xtrain_tensor, Xtest_tensor, Ytrain_tensor, Ytest_tensor = scale_and_convert_to_tensor(X_train, X_test, y_train, y_test)\n",
    "\n",
    "print(\"X train tensor\",Xtrain_tensor.shape)\n",
    "print(\"Y train tensor\",Ytrain_tensor.shape)\n",
    "print(\"X test tensor\",Xtest_tensor.shape)\n",
    "print(\"Y test tensor\",Ytest_tensor.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4. Define BNN training and evaluation pipeline**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model_and_evaluate_classification(Xtrain_tensor, Ytrain_tensor, Xtest_tensor, Ytest_tensor ,\n",
    "                                            layers = [100,20], learning_param = 0.01, kl_weight = 0.01, steps = 100, threshold = 0.50, printStep = True):    \n",
    "    in_features = Xtrain_tensor.shape[1]\n",
    "    batch_size = Xtrain_tensor.shape[0]\n",
    "    \n",
    "    ## Ensure reproducibility\n",
    "    seed = 1\n",
    "    torch.manual_seed(seed)\n",
    "    \n",
    "    # Build model\n",
    "    layer = []\n",
    "    \n",
    "    ## Input layer\n",
    "    layer.append(bnn.BayesLinear(prior_mu=0, prior_sigma=0.1, in_features = in_features, out_features = layers[0]))\n",
    "    layer.append(nn.ReLU())\n",
    "    \n",
    "    ## Hidden layers\n",
    "    for index, neurons in enumerate(layers):\n",
    "        if index != (len(layers)-1):\n",
    "            layer.append(bnn.BayesLinear(prior_mu=0, prior_sigma=0.1, in_features=neurons, out_features=layers[index+1]))\n",
    "            layer.append(nn.ReLU())\n",
    "\n",
    "    ## Output layer\n",
    "    layer.append(bnn.BayesLinear(prior_mu=0, prior_sigma=0.1, in_features=layers[-1], out_features=1))\n",
    "    layer.append(nn.Sigmoid())\n",
    "    \n",
    "    model = nn.Sequential(*layer)\n",
    "    \n",
    "    ### Define Loss - CrossEntropy for classification\n",
    "    cross_entropy_loss = nn.BCELoss()\n",
    "    kl_loss = bnn.BKLLoss(reduction='mean', last_layer_only=False)\n",
    "\n",
    "    ## Define optimiser with learning rate\n",
    "    optimizer = optim.Adam(model.parameters(), lr = learning_param)\n",
    "    \n",
    "    ### Train model\n",
    "    for step in range(steps):\n",
    "        torch.manual_seed(seed)\n",
    "        \n",
    "        pre = model(Xtrain_tensor)\n",
    "        cross_entropy = cross_entropy_loss(pre, Ytrain_tensor.reshape(-1, 1).type(torch.FloatTensor))\n",
    "        kl = kl_loss(model)\n",
    "        total_cost = cross_entropy + kl_weight*kl\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        total_cost.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "        if step%20==0 and printStep:\n",
    "            print('[Step %d]: CE : %.8f, KL : %.8f' % (step , cross_entropy.item(), kl.item()))\n",
    "\n",
    "    train_cross_entropy = cross_entropy.item()\n",
    "    kl_loss = kl.item()\n",
    "    \n",
    "    torch.manual_seed(seed)\n",
    "    ## Performance Evaluation on train\n",
    "    y_predict_train = model(Xtrain_tensor)\n",
    "    y_predict_binary_train = pd.Series(y_predict_train.detach().numpy().flatten()).apply(lambda x: 1 if x >= threshold else 0)\n",
    "    train_accuracy = accuracy_score(Ytrain_tensor.detach().numpy(), y_predict_binary_train)\n",
    "    \n",
    "    ## Performance Evaluation on test\n",
    "    y_predict = model(Xtest_tensor)\n",
    "    y_predict_binary = pd.Series(y_predict.detach().numpy().flatten()).apply(lambda x: 1 if x >= threshold else 0)\n",
    "    test_cross_entropy = cross_entropy_loss(y_predict, Ytest_tensor.reshape(-1, 1).type(torch.FloatTensor)).item()\n",
    "    test_accuracy = accuracy_score(Ytest_tensor.detach().numpy(), y_predict_binary)\n",
    "\n",
    "    return (y_predict_binary , train_cross_entropy, test_cross_entropy, train_accuracy, test_accuracy, kl_loss)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**5. Perform Grid Search**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Layer:  [32, 16]\n",
      "-- Learning Param:  0.001\n",
      "-- Learning Param:  0.011\n",
      "-- Learning Param:  0.021\n",
      "-- Learning Param:  0.032\n",
      "-- Learning Param:  0.042\n",
      "-- Learning Param:  0.052\n",
      "-- Learning Param:  0.062\n",
      "-- Learning Param:  0.072\n",
      "-- Learning Param:  0.082\n",
      "-- Learning Param:  0.093\n",
      "-- Learning Param:  0.103\n",
      "-- Learning Param:  0.113\n",
      "-- Learning Param:  0.123\n",
      "-- Learning Param:  0.133\n",
      "-- Learning Param:  0.144\n",
      "-- Learning Param:  0.154\n",
      "-- Learning Param:  0.164\n",
      "-- Learning Param:  0.174\n",
      "-- Learning Param:  0.184\n",
      "-- Learning Param:  0.194\n",
      "-- Learning Param:  0.205\n",
      "-- Learning Param:  0.215\n",
      "-- Learning Param:  0.225\n",
      "-- Learning Param:  0.235\n",
      "-- Learning Param:  0.245\n",
      "-- Learning Param:  0.256\n",
      "-- Learning Param:  0.266\n",
      "-- Learning Param:  0.276\n",
      "-- Learning Param:  0.286\n",
      "-- Learning Param:  0.296\n",
      "-- Learning Param:  0.307\n",
      "-- Learning Param:  0.317\n",
      "-- Learning Param:  0.327\n",
      "-- Learning Param:  0.337\n",
      "-- Learning Param:  0.347\n",
      "-- Learning Param:  0.357\n",
      "-- Learning Param:  0.368\n",
      "-- Learning Param:  0.378\n",
      "-- Learning Param:  0.388\n",
      "-- Learning Param:  0.398\n",
      "-- Learning Param:  0.408\n",
      "-- Learning Param:  0.419\n",
      "-- Learning Param:  0.429\n",
      "-- Learning Param:  0.439\n",
      "-- Learning Param:  0.449\n",
      "-- Learning Param:  0.459\n",
      "-- Learning Param:  0.469\n",
      "-- Learning Param:  0.48\n",
      "-- Learning Param:  0.49\n",
      "-- Learning Param:  0.5\n",
      "--- Layer:  [32, 8]\n",
      "-- Learning Param:  0.001\n",
      "-- Learning Param:  0.011\n",
      "-- Learning Param:  0.021\n",
      "-- Learning Param:  0.032\n",
      "-- Learning Param:  0.042\n",
      "-- Learning Param:  0.052\n",
      "-- Learning Param:  0.062\n",
      "-- Learning Param:  0.072\n",
      "-- Learning Param:  0.082\n",
      "-- Learning Param:  0.093\n",
      "-- Learning Param:  0.103\n",
      "-- Learning Param:  0.113\n",
      "-- Learning Param:  0.123\n",
      "-- Learning Param:  0.133\n",
      "-- Learning Param:  0.144\n",
      "-- Learning Param:  0.154\n",
      "-- Learning Param:  0.164\n",
      "-- Learning Param:  0.174\n",
      "-- Learning Param:  0.184\n",
      "-- Learning Param:  0.194\n",
      "-- Learning Param:  0.205\n",
      "-- Learning Param:  0.215\n",
      "-- Learning Param:  0.225\n",
      "-- Learning Param:  0.235\n",
      "-- Learning Param:  0.245\n",
      "-- Learning Param:  0.256\n",
      "-- Learning Param:  0.266\n",
      "-- Learning Param:  0.276\n",
      "-- Learning Param:  0.286\n",
      "-- Learning Param:  0.296\n",
      "-- Learning Param:  0.307\n",
      "-- Learning Param:  0.317\n",
      "-- Learning Param:  0.327\n",
      "-- Learning Param:  0.337\n",
      "-- Learning Param:  0.347\n",
      "-- Learning Param:  0.357\n",
      "-- Learning Param:  0.368\n",
      "-- Learning Param:  0.378\n",
      "-- Learning Param:  0.388\n",
      "-- Learning Param:  0.398\n",
      "-- Learning Param:  0.408\n",
      "-- Learning Param:  0.419\n",
      "-- Learning Param:  0.429\n",
      "-- Learning Param:  0.439\n",
      "-- Learning Param:  0.449\n",
      "-- Learning Param:  0.459\n",
      "-- Learning Param:  0.469\n",
      "-- Learning Param:  0.48\n",
      "-- Learning Param:  0.49\n",
      "-- Learning Param:  0.5\n",
      "--- Layer:  [32, 16, 8]\n",
      "-- Learning Param:  0.001\n",
      "-- Learning Param:  0.011\n",
      "-- Learning Param:  0.021\n",
      "-- Learning Param:  0.032\n",
      "-- Learning Param:  0.042\n",
      "-- Learning Param:  0.052\n",
      "-- Learning Param:  0.062\n",
      "-- Learning Param:  0.072\n",
      "-- Learning Param:  0.082\n",
      "-- Learning Param:  0.093\n",
      "-- Learning Param:  0.103\n",
      "-- Learning Param:  0.113\n",
      "-- Learning Param:  0.123\n",
      "-- Learning Param:  0.133\n",
      "-- Learning Param:  0.144\n",
      "-- Learning Param:  0.154\n",
      "-- Learning Param:  0.164\n",
      "-- Learning Param:  0.174\n",
      "-- Learning Param:  0.184\n",
      "-- Learning Param:  0.194\n",
      "-- Learning Param:  0.205\n",
      "-- Learning Param:  0.215\n",
      "-- Learning Param:  0.225\n",
      "-- Learning Param:  0.235\n",
      "-- Learning Param:  0.245\n",
      "-- Learning Param:  0.256\n",
      "-- Learning Param:  0.266\n",
      "-- Learning Param:  0.276\n",
      "-- Learning Param:  0.286\n",
      "-- Learning Param:  0.296\n",
      "-- Learning Param:  0.307\n",
      "-- Learning Param:  0.317\n",
      "-- Learning Param:  0.327\n",
      "-- Learning Param:  0.337\n",
      "-- Learning Param:  0.347\n",
      "-- Learning Param:  0.357\n",
      "-- Learning Param:  0.368\n",
      "-- Learning Param:  0.378\n",
      "-- Learning Param:  0.388\n",
      "-- Learning Param:  0.398\n",
      "-- Learning Param:  0.408\n",
      "-- Learning Param:  0.419\n",
      "-- Learning Param:  0.429\n",
      "-- Learning Param:  0.439\n",
      "-- Learning Param:  0.449\n",
      "-- Learning Param:  0.459\n",
      "-- Learning Param:  0.469\n",
      "-- Learning Param:  0.48\n",
      "-- Learning Param:  0.49\n",
      "-- Learning Param:  0.5\n",
      "--- Layer:  [32, 20, 10]\n",
      "-- Learning Param:  0.001\n",
      "-- Learning Param:  0.011\n",
      "-- Learning Param:  0.021\n",
      "-- Learning Param:  0.032\n",
      "-- Learning Param:  0.042\n",
      "-- Learning Param:  0.052\n",
      "-- Learning Param:  0.062\n",
      "-- Learning Param:  0.072\n",
      "-- Learning Param:  0.082\n",
      "-- Learning Param:  0.093\n",
      "-- Learning Param:  0.103\n",
      "-- Learning Param:  0.113\n",
      "-- Learning Param:  0.123\n",
      "-- Learning Param:  0.133\n",
      "-- Learning Param:  0.144\n",
      "-- Learning Param:  0.154\n",
      "-- Learning Param:  0.164\n",
      "-- Learning Param:  0.174\n",
      "-- Learning Param:  0.184\n",
      "-- Learning Param:  0.194\n",
      "-- Learning Param:  0.205\n",
      "-- Learning Param:  0.215\n",
      "-- Learning Param:  0.225\n",
      "-- Learning Param:  0.235\n",
      "-- Learning Param:  0.245\n",
      "-- Learning Param:  0.256\n",
      "-- Learning Param:  0.266\n",
      "-- Learning Param:  0.276\n",
      "-- Learning Param:  0.286\n",
      "-- Learning Param:  0.296\n",
      "-- Learning Param:  0.307\n",
      "-- Learning Param:  0.317\n",
      "-- Learning Param:  0.327\n",
      "-- Learning Param:  0.337\n",
      "-- Learning Param:  0.347\n",
      "-- Learning Param:  0.357\n",
      "-- Learning Param:  0.368\n",
      "-- Learning Param:  0.378\n",
      "-- Learning Param:  0.388\n",
      "-- Learning Param:  0.398\n",
      "-- Learning Param:  0.408\n",
      "-- Learning Param:  0.419\n",
      "-- Learning Param:  0.429\n",
      "-- Learning Param:  0.439\n",
      "-- Learning Param:  0.449\n",
      "-- Learning Param:  0.459\n",
      "-- Learning Param:  0.469\n",
      "-- Learning Param:  0.48\n",
      "-- Learning Param:  0.49\n",
      "-- Learning Param:  0.5\n",
      "Complete\n"
     ]
    }
   ],
   "source": [
    "### Hyperparameter Tuning --> find best parameters\n",
    "train_accuracy_list = []\n",
    "test_accuracy_list = []\n",
    "combination = []\n",
    "\n",
    "learning_param_list = pd.Series(np.linspace(0.001,0.5,50)).apply(lambda x: round(x,3))\n",
    "kl_weight =  pd.Series(np.linspace(0.001,0.5,50)).apply(lambda x: round(x,3))\n",
    "layers_list = ([32,16],[32,8],[32,16,8],[32,20,10])\n",
    "\n",
    "for layer in layers_list:\n",
    "    print(\"--- Layer: \", layer)\n",
    "    for lr in learning_param_list:\n",
    "        print(\"-- Learning Param: \", lr)\n",
    "        for kl in kl_weight:\n",
    "            combination.append(\"layer: {} lr: {} kl: {}\".format(layer,lr,kl))\n",
    "            _ , _, _, train_accuracy, test_accuracy, _ = train_model_and_evaluate_classification(Xtrain_tensor, Ytrain_tensor, Xtest_tensor, Ytest_tensor, layer, learning_param = lr, kl_weight = kl, steps = 100, printStep = False)\n",
    "            train_accuracy_list.append(train_accuracy)\n",
    "            test_accuracy_list.append(test_accuracy)\n",
    "            \n",
    "print(\"Complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1296        layer: [32, 16] lr: 0.256 kl: 0.469\n",
       "3303         layer: [32, 8] lr: 0.164 kl: 0.032\n",
       "3311         layer: [32, 8] lr: 0.164 kl: 0.113\n",
       "5147      layer: [32, 16, 8] lr: 0.021 kl: 0.48\n",
       "7383      layer: [32, 16, 8] lr: 0.48 kl: 0.337\n",
       "7721    layer: [32, 20, 10] lr: 0.042 kl: 0.215\n",
       "Name: Combination, dtype: object"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = pd.DataFrame({\"Combination\": combination,\"Train Accuracy\":train_accuracy_list, \"Test Accuracy\":test_accuracy_list})\n",
    "results.to_csv(\"Combinations_classification_withoutSentiments&lagged2.csv\")\n",
    "\n",
    "## Find the hyperparameters with gives the lowest test RMSE\n",
    "results[results['Test Accuracy'] ==  results['Test Accuracy'].max()]['Combination']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy:  0.9130434782608695\n",
      "Test Accuracy:  0.7692307692307693\n"
     ]
    }
   ],
   "source": [
    "layer = [32, 8]\n",
    "lr = 0.164\n",
    "kl = 0.113\n",
    "\n",
    "_ , _, _, train_accuracy, test_accuracy, _ = train_model_and_evaluate_classification(Xtrain_tensor, Ytrain_tensor, Xtest_tensor, Ytest_tensor, layer, learning_param = lr, kl_weight = kl, steps = 100, printStep = False)\n",
    "print(\"Train Accuracy: \",train_accuracy)\n",
    "print(\"Test Accuracy: \",test_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**7. Retrain the model with selected hyperparameters and all train data available**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Helper function for retraining the model with all data available\n",
    "def train_model(Xtrain_tensor, Ytrain_tensor, layers = [100,20], learning_param = 0.01, kl_weight = 0.01, steps = 100, threshold = 0.50):    \n",
    "    \"\"\" \n",
    "    Trains model and returns predictions on entire dataset.\n",
    "    \"\"\"\n",
    "    \n",
    "    in_features = Xtrain_tensor.shape[1]\n",
    "    batch_size = Xtrain_tensor.shape[0]\n",
    "    \n",
    "    ## Ensure reproducibility\n",
    "    seed = 1\n",
    "    torch.manual_seed(seed)\n",
    "    \n",
    "    # Build model\n",
    "    layer = []\n",
    "    \n",
    "    ## Input layer\n",
    "    layer.append(bnn.BayesLinear(prior_mu=0, prior_sigma=0.1, in_features = in_features, out_features = layers[0]))\n",
    "    layer.append(nn.ReLU())\n",
    "    \n",
    "    ## Hidden layers\n",
    "    for index, neurons in enumerate(layers):\n",
    "        if index != (len(layers)-1):\n",
    "            layer.append(bnn.BayesLinear(prior_mu=0, prior_sigma=0.1, in_features=neurons, out_features=layers[index+1]))\n",
    "            layer.append(nn.ReLU())\n",
    "\n",
    "    ## Output layer\n",
    "    layer.append(bnn.BayesLinear(prior_mu=0, prior_sigma=0.1, in_features=layers[-1], out_features=1))\n",
    "    layer.append(nn.Sigmoid())\n",
    "    \n",
    "    model = nn.Sequential(*layer)\n",
    "    \n",
    "    ## Print Model\n",
    "    #print(summary(model,(batch_size,in_features)))\n",
    "    #print(model)\n",
    "    \n",
    "    ### Define Loss - CrossEntropy for classification\n",
    "    cross_entropy_loss = nn.BCELoss()\n",
    "    kl_loss = bnn.BKLLoss(reduction='mean', last_layer_only=False)\n",
    "\n",
    "    ## Define optimiser with learning rate\n",
    "    optimizer = optim.Adam(model.parameters(), lr = learning_param)\n",
    "    \n",
    "    ### Train model\n",
    "    for step in range(steps):\n",
    "        torch.manual_seed(seed)\n",
    "        \n",
    "        pre = model(Xtrain_tensor)\n",
    "        cross_entropy = cross_entropy_loss(pre, Ytrain_tensor.reshape(-1, 1).type(torch.FloatTensor))\n",
    "        kl = kl_loss(model)\n",
    "        total_cost = cross_entropy + kl_weight*kl\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        total_cost.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "    pre_binary = pd.Series(pre.detach().numpy().flatten()).apply(lambda x: 1 if x >= threshold else 0)\n",
    "    \n",
    "    return (model, pre_binary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Obtain all available data\n",
    "x_refit = data.drop([\"date\", \"class_y\"], axis = 1)\n",
    "y_refit = data[\"class_y\"]\n",
    "\n",
    "### Used helper standardise function to create test and train but Xtrain_tensor is equal to Xtest_tensor\n",
    "Xtrain_tensor, Xtest_tensor, Ytrain_tensor, Ytest_tensor = scale_and_convert_to_tensor(x_refit, x_refit, y_refit, y_refit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, y_predict = train_model(Xtrain_tensor, Ytrain_tensor, layers = layer, learning_param = lr, kl_weight = kl, steps = 100)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pd.DataFrame(y_predict).to_csv(\"out.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## With Sentiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1. Create Lags**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_lags = {\"Adj_Close_BTC-USD\" : 2, \n",
    "                'Open_BTC-USD': 1, \n",
    "                'Low_BTC-USD': 1, \n",
    "                'High_BTC-USD': 1,\n",
    "                \"Volume_BTC-USD\" : 1, \n",
    "                \"Adj_Close_SPY\" : 1,\n",
    "                \"Adj_Close_GLD\" : 1,\n",
    "                \"Adj_Close_CHFUSD=X\" : 1,\n",
    "                \"Adj_Close_CNYUSD=X\" : 1,\n",
    "                \"Adj_Close_EURUSD=X\" : 1,\n",
    "                \"Adj_Close_GBPUSD=X\" : 1,\n",
    "                \"Adj_Close_JPYUSD=X\" : 1,\n",
    "                \"blockchain_transactions_per_block\" : 1,\n",
    "                \"blockchain_hash_rates\" : 1,\n",
    "                \"coindesk_sentiment\" : 1,\n",
    "                \"num_of_coindesk_posts\" : 1,\n",
    "                \"reddit_comments_sentiments\" : 1,\n",
    "                \"top_50_reddit_posts_sentiments\" : 1}\n",
    "\n",
    "data = lag(df, feature_lags)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2. Handle Train-test split**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data[data[\"date\"] >= \"1/1/2021\"]\n",
    "\n",
    "train = data[data[\"date\"] <= \"2021-03-10\"]\n",
    "test = data[(data[\"date\"] > \"2021-03-10\") & (data[\"date\"] <= \"2021-04-5\")]\n",
    "\n",
    "X_train = train.drop([\"date\", \"class_y\"], axis = 1)\n",
    "y_train = train[\"class_y\"]\n",
    "\n",
    "X_test = test.drop([\"date\", \"class_y\"], axis = 1)\n",
    "y_test = test[\"class_y\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Adj_Close_BTC-USD_lag1</th>\n",
       "      <th>Adj_Close_BTC-USD_lag2</th>\n",
       "      <th>Open_BTC-USD_lag1</th>\n",
       "      <th>Low_BTC-USD_lag1</th>\n",
       "      <th>High_BTC-USD_lag1</th>\n",
       "      <th>Volume_BTC-USD_lag1</th>\n",
       "      <th>Adj_Close_SPY_lag1</th>\n",
       "      <th>Adj_Close_GLD_lag1</th>\n",
       "      <th>Adj_Close_CHFUSD=X_lag1</th>\n",
       "      <th>Adj_Close_CNYUSD=X_lag1</th>\n",
       "      <th>Adj_Close_EURUSD=X_lag1</th>\n",
       "      <th>Adj_Close_GBPUSD=X_lag1</th>\n",
       "      <th>Adj_Close_JPYUSD=X_lag1</th>\n",
       "      <th>blockchain_transactions_per_block_lag1</th>\n",
       "      <th>blockchain_hash_rates_lag1</th>\n",
       "      <th>coindesk_sentiment_lag1</th>\n",
       "      <th>num_of_coindesk_posts_lag1</th>\n",
       "      <th>reddit_comments_sentiments_lag1</th>\n",
       "      <th>top_50_reddit_posts_sentiments_lag1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>29001.72070</td>\n",
       "      <td>28840.95313</td>\n",
       "      <td>28841.57422</td>\n",
       "      <td>28201.99219</td>\n",
       "      <td>29244.87695</td>\n",
       "      <td>4.675496e+10</td>\n",
       "      <td>372.659454</td>\n",
       "      <td>178.360001</td>\n",
       "      <td>1.134327</td>\n",
       "      <td>0.153323</td>\n",
       "      <td>1.229990</td>\n",
       "      <td>1.363066</td>\n",
       "      <td>0.009697</td>\n",
       "      <td>2077.819277</td>\n",
       "      <td>137514350.9</td>\n",
       "      <td>0.235017</td>\n",
       "      <td>18.0</td>\n",
       "      <td>0.127256</td>\n",
       "      <td>0.466726</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>29374.15234</td>\n",
       "      <td>29001.72070</td>\n",
       "      <td>28994.00977</td>\n",
       "      <td>28803.58594</td>\n",
       "      <td>29600.62695</td>\n",
       "      <td>4.073030e+10</td>\n",
       "      <td>372.659454</td>\n",
       "      <td>178.360001</td>\n",
       "      <td>1.113462</td>\n",
       "      <td>0.153099</td>\n",
       "      <td>1.218027</td>\n",
       "      <td>1.367301</td>\n",
       "      <td>0.009687</td>\n",
       "      <td>1732.080537</td>\n",
       "      <td>142734576.9</td>\n",
       "      <td>0.254295</td>\n",
       "      <td>18.0</td>\n",
       "      <td>0.144794</td>\n",
       "      <td>0.551627</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>32127.26758</td>\n",
       "      <td>29374.15234</td>\n",
       "      <td>29376.45508</td>\n",
       "      <td>29091.18164</td>\n",
       "      <td>33155.11719</td>\n",
       "      <td>6.786542e+10</td>\n",
       "      <td>372.659454</td>\n",
       "      <td>178.360001</td>\n",
       "      <td>1.113462</td>\n",
       "      <td>0.153099</td>\n",
       "      <td>1.218027</td>\n",
       "      <td>1.367301</td>\n",
       "      <td>0.009687</td>\n",
       "      <td>1967.622517</td>\n",
       "      <td>143322827.6</td>\n",
       "      <td>0.118618</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.242964</td>\n",
       "      <td>0.538810</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>32782.02344</td>\n",
       "      <td>32127.26758</td>\n",
       "      <td>32129.40820</td>\n",
       "      <td>32052.31641</td>\n",
       "      <td>34608.55859</td>\n",
       "      <td>7.866524e+10</td>\n",
       "      <td>372.659454</td>\n",
       "      <td>178.360001</td>\n",
       "      <td>1.113462</td>\n",
       "      <td>0.153099</td>\n",
       "      <td>1.218027</td>\n",
       "      <td>1.367301</td>\n",
       "      <td>0.009687</td>\n",
       "      <td>2272.886076</td>\n",
       "      <td>145103346.2</td>\n",
       "      <td>0.194716</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.139765</td>\n",
       "      <td>0.379505</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>31971.91406</td>\n",
       "      <td>32782.02344</td>\n",
       "      <td>32810.94922</td>\n",
       "      <td>28722.75586</td>\n",
       "      <td>33440.21875</td>\n",
       "      <td>8.116348e+10</td>\n",
       "      <td>367.586090</td>\n",
       "      <td>182.330002</td>\n",
       "      <td>1.132375</td>\n",
       "      <td>0.153092</td>\n",
       "      <td>1.225070</td>\n",
       "      <td>1.368420</td>\n",
       "      <td>0.009686</td>\n",
       "      <td>2160.312139</td>\n",
       "      <td>146350014.1</td>\n",
       "      <td>0.209840</td>\n",
       "      <td>14.0</td>\n",
       "      <td>0.177962</td>\n",
       "      <td>0.235787</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Adj_Close_BTC-USD_lag1  Adj_Close_BTC-USD_lag2  Open_BTC-USD_lag1  \\\n",
       "18             29001.72070             28840.95313        28841.57422   \n",
       "19             29374.15234             29001.72070        28994.00977   \n",
       "20             32127.26758             29374.15234        29376.45508   \n",
       "21             32782.02344             32127.26758        32129.40820   \n",
       "22             31971.91406             32782.02344        32810.94922   \n",
       "\n",
       "    Low_BTC-USD_lag1  High_BTC-USD_lag1  Volume_BTC-USD_lag1  \\\n",
       "18       28201.99219        29244.87695         4.675496e+10   \n",
       "19       28803.58594        29600.62695         4.073030e+10   \n",
       "20       29091.18164        33155.11719         6.786542e+10   \n",
       "21       32052.31641        34608.55859         7.866524e+10   \n",
       "22       28722.75586        33440.21875         8.116348e+10   \n",
       "\n",
       "    Adj_Close_SPY_lag1  Adj_Close_GLD_lag1  Adj_Close_CHFUSD=X_lag1  \\\n",
       "18          372.659454          178.360001                 1.134327   \n",
       "19          372.659454          178.360001                 1.113462   \n",
       "20          372.659454          178.360001                 1.113462   \n",
       "21          372.659454          178.360001                 1.113462   \n",
       "22          367.586090          182.330002                 1.132375   \n",
       "\n",
       "    Adj_Close_CNYUSD=X_lag1  Adj_Close_EURUSD=X_lag1  Adj_Close_GBPUSD=X_lag1  \\\n",
       "18                 0.153323                 1.229990                 1.363066   \n",
       "19                 0.153099                 1.218027                 1.367301   \n",
       "20                 0.153099                 1.218027                 1.367301   \n",
       "21                 0.153099                 1.218027                 1.367301   \n",
       "22                 0.153092                 1.225070                 1.368420   \n",
       "\n",
       "    Adj_Close_JPYUSD=X_lag1  blockchain_transactions_per_block_lag1  \\\n",
       "18                 0.009697                             2077.819277   \n",
       "19                 0.009687                             1732.080537   \n",
       "20                 0.009687                             1967.622517   \n",
       "21                 0.009687                             2272.886076   \n",
       "22                 0.009686                             2160.312139   \n",
       "\n",
       "    blockchain_hash_rates_lag1  coindesk_sentiment_lag1  \\\n",
       "18                 137514350.9                 0.235017   \n",
       "19                 142734576.9                 0.254295   \n",
       "20                 143322827.6                 0.118618   \n",
       "21                 145103346.2                 0.194716   \n",
       "22                 146350014.1                 0.209840   \n",
       "\n",
       "    num_of_coindesk_posts_lag1  reddit_comments_sentiments_lag1  \\\n",
       "18                        18.0                         0.127256   \n",
       "19                        18.0                         0.144794   \n",
       "20                         6.0                         0.242964   \n",
       "21                         5.0                         0.139765   \n",
       "22                        14.0                         0.177962   \n",
       "\n",
       "    top_50_reddit_posts_sentiments_lag1  \n",
       "18                             0.466726  \n",
       "19                             0.551627  \n",
       "20                             0.538810  \n",
       "21                             0.379505  \n",
       "22                             0.235787  "
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3. Standardise dataset and transform to tensors**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X train tensor torch.Size([69, 19])\n",
      "Y train tensor torch.Size([69])\n",
      "X test tensor torch.Size([26, 19])\n",
      "Y test tensor torch.Size([26])\n"
     ]
    }
   ],
   "source": [
    "Xtrain_tensor, Xtest_tensor, Ytrain_tensor, Ytest_tensor = scale_and_convert_to_tensor(X_train, X_test, y_train, y_test)\n",
    "\n",
    "print(\"X train tensor\",Xtrain_tensor.shape)\n",
    "print(\"Y train tensor\",Ytrain_tensor.shape)\n",
    "print(\"X test tensor\",Xtest_tensor.shape)\n",
    "print(\"Y test tensor\",Ytest_tensor.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4. Perform Gridsearch**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Layer:  [32, 16]\n",
      "-- Learning Param:  0.001\n",
      "-- Learning Param:  0.011\n",
      "-- Learning Param:  0.021\n",
      "-- Learning Param:  0.032\n",
      "-- Learning Param:  0.042\n",
      "-- Learning Param:  0.052\n",
      "-- Learning Param:  0.062\n",
      "-- Learning Param:  0.072\n",
      "-- Learning Param:  0.082\n",
      "-- Learning Param:  0.093\n",
      "-- Learning Param:  0.103\n",
      "-- Learning Param:  0.113\n",
      "-- Learning Param:  0.123\n",
      "-- Learning Param:  0.133\n",
      "-- Learning Param:  0.144\n",
      "-- Learning Param:  0.154\n",
      "-- Learning Param:  0.164\n",
      "-- Learning Param:  0.174\n",
      "-- Learning Param:  0.184\n",
      "-- Learning Param:  0.194\n",
      "-- Learning Param:  0.205\n",
      "-- Learning Param:  0.215\n",
      "-- Learning Param:  0.225\n",
      "-- Learning Param:  0.235\n",
      "-- Learning Param:  0.245\n",
      "-- Learning Param:  0.256\n",
      "-- Learning Param:  0.266\n",
      "-- Learning Param:  0.276\n",
      "-- Learning Param:  0.286\n",
      "-- Learning Param:  0.296\n",
      "-- Learning Param:  0.307\n",
      "-- Learning Param:  0.317\n",
      "-- Learning Param:  0.327\n",
      "-- Learning Param:  0.337\n",
      "-- Learning Param:  0.347\n",
      "-- Learning Param:  0.357\n",
      "-- Learning Param:  0.368\n",
      "-- Learning Param:  0.378\n",
      "-- Learning Param:  0.388\n",
      "-- Learning Param:  0.398\n",
      "-- Learning Param:  0.408\n",
      "-- Learning Param:  0.419\n",
      "-- Learning Param:  0.429\n",
      "-- Learning Param:  0.439\n",
      "-- Learning Param:  0.449\n",
      "-- Learning Param:  0.459\n",
      "-- Learning Param:  0.469\n",
      "-- Learning Param:  0.48\n",
      "-- Learning Param:  0.49\n",
      "-- Learning Param:  0.5\n",
      "--- Layer:  [32, 8]\n",
      "-- Learning Param:  0.001\n",
      "-- Learning Param:  0.011\n",
      "-- Learning Param:  0.021\n",
      "-- Learning Param:  0.032\n",
      "-- Learning Param:  0.042\n",
      "-- Learning Param:  0.052\n",
      "-- Learning Param:  0.062\n",
      "-- Learning Param:  0.072\n",
      "-- Learning Param:  0.082\n",
      "-- Learning Param:  0.093\n",
      "-- Learning Param:  0.103\n",
      "-- Learning Param:  0.113\n",
      "-- Learning Param:  0.123\n",
      "-- Learning Param:  0.133\n",
      "-- Learning Param:  0.144\n",
      "-- Learning Param:  0.154\n",
      "-- Learning Param:  0.164\n",
      "-- Learning Param:  0.174\n",
      "-- Learning Param:  0.184\n",
      "-- Learning Param:  0.194\n",
      "-- Learning Param:  0.205\n",
      "-- Learning Param:  0.215\n",
      "-- Learning Param:  0.225\n",
      "-- Learning Param:  0.235\n",
      "-- Learning Param:  0.245\n",
      "-- Learning Param:  0.256\n",
      "-- Learning Param:  0.266\n",
      "-- Learning Param:  0.276\n",
      "-- Learning Param:  0.286\n",
      "-- Learning Param:  0.296\n",
      "-- Learning Param:  0.307\n",
      "-- Learning Param:  0.317\n",
      "-- Learning Param:  0.327\n",
      "-- Learning Param:  0.337\n",
      "-- Learning Param:  0.347\n",
      "-- Learning Param:  0.357\n",
      "-- Learning Param:  0.368\n",
      "-- Learning Param:  0.378\n",
      "-- Learning Param:  0.388\n",
      "-- Learning Param:  0.398\n",
      "-- Learning Param:  0.408\n",
      "-- Learning Param:  0.419\n",
      "-- Learning Param:  0.429\n",
      "-- Learning Param:  0.439\n",
      "-- Learning Param:  0.449\n",
      "-- Learning Param:  0.459\n",
      "-- Learning Param:  0.469\n",
      "-- Learning Param:  0.48\n",
      "-- Learning Param:  0.49\n",
      "-- Learning Param:  0.5\n",
      "--- Layer:  [32, 16, 8]\n",
      "-- Learning Param:  0.001\n",
      "-- Learning Param:  0.011\n",
      "-- Learning Param:  0.021\n",
      "-- Learning Param:  0.032\n",
      "-- Learning Param:  0.042\n",
      "-- Learning Param:  0.052\n",
      "-- Learning Param:  0.062\n",
      "-- Learning Param:  0.072\n",
      "-- Learning Param:  0.082\n",
      "-- Learning Param:  0.093\n",
      "-- Learning Param:  0.103\n",
      "-- Learning Param:  0.113\n",
      "-- Learning Param:  0.123\n",
      "-- Learning Param:  0.133\n",
      "-- Learning Param:  0.144\n",
      "-- Learning Param:  0.154\n",
      "-- Learning Param:  0.164\n",
      "-- Learning Param:  0.174\n",
      "-- Learning Param:  0.184\n",
      "-- Learning Param:  0.194\n",
      "-- Learning Param:  0.205\n",
      "-- Learning Param:  0.215\n",
      "-- Learning Param:  0.225\n",
      "-- Learning Param:  0.235\n",
      "-- Learning Param:  0.245\n",
      "-- Learning Param:  0.256\n",
      "-- Learning Param:  0.266\n",
      "-- Learning Param:  0.276\n",
      "-- Learning Param:  0.286\n",
      "-- Learning Param:  0.296\n",
      "-- Learning Param:  0.307\n",
      "-- Learning Param:  0.317\n",
      "-- Learning Param:  0.327\n",
      "-- Learning Param:  0.337\n",
      "-- Learning Param:  0.347\n",
      "-- Learning Param:  0.357\n",
      "-- Learning Param:  0.368\n",
      "-- Learning Param:  0.378\n",
      "-- Learning Param:  0.388\n",
      "-- Learning Param:  0.398\n",
      "-- Learning Param:  0.408\n",
      "-- Learning Param:  0.419\n",
      "-- Learning Param:  0.429\n",
      "-- Learning Param:  0.439\n",
      "-- Learning Param:  0.449\n",
      "-- Learning Param:  0.459\n",
      "-- Learning Param:  0.469\n",
      "-- Learning Param:  0.48\n",
      "-- Learning Param:  0.49\n",
      "-- Learning Param:  0.5\n",
      "Complete\n"
     ]
    }
   ],
   "source": [
    "### Hyperparameter Tuning --> find best parameters\n",
    "train_accuracy_list = []\n",
    "test_accuracy_list = []\n",
    "combination = []\n",
    "\n",
    "learning_param_list = pd.Series(np.linspace(0.001,0.5,50)).apply(lambda x: round(x,3))\n",
    "kl_weight =  pd.Series(np.linspace(0.001,0.5,50)).apply(lambda x: round(x,3))\n",
    "layers_list = ([32,16],[32,8],[32,16,8])\n",
    "\n",
    "for layer in layers_list:\n",
    "    print(\"--- Layer: \", layer)\n",
    "    for lr in learning_param_list:\n",
    "        print(\"-- Learning Param: \", lr)\n",
    "        for kl in kl_weight:\n",
    "            combination.append(\"layer: {} lr: {} kl: {}\".format(layer,lr,kl))\n",
    "            _ , _, _, train_accuracy, test_accuracy, _ = train_model_and_evaluate_classification(Xtrain_tensor, Ytrain_tensor, Xtest_tensor, Ytest_tensor, layer, learning_param = lr, kl_weight = kl, steps = 100, printStep = False)\n",
    "            train_accuracy_list.append(train_accuracy)\n",
    "            test_accuracy_list.append(test_accuracy)\n",
    "            \n",
    "print(\"Complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1028    layer: [32, 16] lr: 0.205 kl: 0.286\n",
       "Name: Combination, dtype: object"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = pd.DataFrame({\"Combination\": combination,\"Train Accuracy\":train_accuracy_list, \"Test Accuracy\":test_accuracy_list})\n",
    "results.to_csv(\"Combinations_classification_withSentiments&lagged2.csv\")\n",
    "\n",
    "## Find the hyperparameters with gives the lowest test RMSE\n",
    "results[results['Test Accuracy'] ==  results['Test Accuracy'].max()]['Combination']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy:  0.9710144927536232\n",
      "Test Accuracy:  0.8846153846153846\n"
     ]
    }
   ],
   "source": [
    "layer = [32, 16]\n",
    "lr = 0.205\n",
    "kl = 0.286\n",
    "\n",
    "_ , _, _, train_accuracy, test_accuracy, _ = train_model_and_evaluate_classification(Xtrain_tensor, Ytrain_tensor, Xtest_tensor, Ytest_tensor, layer, learning_param = lr, kl_weight = kl, steps = 100, printStep = False)\n",
    "print(\"Train Accuracy: \",train_accuracy)\n",
    "print(\"Test Accuracy: \",test_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**5. Retrain the model with selected hyperparameters and all data available**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Obtain all available data\n",
    "x_refit = data.drop([\"date\", \"class_y\"], axis = 1)\n",
    "y_refit = data[\"class_y\"]\n",
    "\n",
    "### Used helper standardise function to create test and train but Xtrain_tensor is equal to Xtest_tensor\n",
    "Xtrain_tensor, Xtest_tensor, Ytrain_tensor, Ytest_tensor = scale_and_convert_to_tensor(x_refit, x_refit, y_refit, y_refit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, y_predict = train_model(Xtrain_tensor, Ytrain_tensor, layers = layer, learning_param = lr, kl_weight = kl, steps = 100)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pd.DataFrame(y_predict).to_csv(\"out.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
